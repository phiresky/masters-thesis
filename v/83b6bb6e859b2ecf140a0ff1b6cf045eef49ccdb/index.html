<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Robin Ruede" />
  <meta name="dcterms.date" content="2021-07-22" />
  <meta name="keywords" content="MARL, multi-agent, mreinforcement learning, Bayesian conditioning" />
  <title>Bayesian and Attentive Aggregation for Cooperative Multi-Agent Deep Reinforcement Learning</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <!--
  Manubot generated metadata rendered from header-includes-template.html.
  Suggest improvements at https://github.com/manubot/manubot/blob/main/manubot/process/header-includes-template.html
  -->
  <meta name="dc.format" content="text/html" />
  <meta name="dc.title" content="Bayesian and Attentive Aggregation for Cooperative Multi-Agent Deep Reinforcement Learning" />
  <meta name="citation_title" content="Bayesian and Attentive Aggregation for Cooperative Multi-Agent Deep Reinforcement Learning" />
  <meta property="og:title" content="Bayesian and Attentive Aggregation for Cooperative Multi-Agent Deep Reinforcement Learning" />
  <meta property="twitter:title" content="Bayesian and Attentive Aggregation for Cooperative Multi-Agent Deep Reinforcement Learning" />
  <meta name="dc.date" content="2021-07-22" />
  <meta name="citation_publication_date" content="2021-07-22" />
  <meta name="dc.language" content="en-US" />
  <meta name="citation_language" content="en-US" />
  <meta name="dc.relation.ispartof" content="Manubot" />
  <meta name="dc.publisher" content="Manubot" />
  <meta name="citation_journal_title" content="Manubot" />
  <meta name="citation_technical_report_institution" content="Manubot" />
  <meta name="citation_author" content="Robin Ruede" />
  <meta name="citation_author_institution" content="Autonomous Learning Robots Lab, Karlsruhe Institute of Technology" />
  <link rel="canonical" href="https://phiresky.github.io/masters-thesis/" />
  <meta property="og:url" content="https://phiresky.github.io/masters-thesis/" />
  <meta property="twitter:url" content="https://phiresky.github.io/masters-thesis/" />
  <meta name="citation_fulltext_html_url" content="https://phiresky.github.io/masters-thesis/" />
  <meta name="citation_pdf_url" content="https://phiresky.github.io/masters-thesis/manuscript.pdf" />
  <link rel="alternate" type="application/pdf" href="https://phiresky.github.io/masters-thesis/manuscript.pdf" />
  <link rel="alternate" type="text/html" href="https://phiresky.github.io/masters-thesis/v/83b6bb6e859b2ecf140a0ff1b6cf045eef49ccdb/" />
  <meta name="manubot_html_url_versioned" content="https://phiresky.github.io/masters-thesis/v/83b6bb6e859b2ecf140a0ff1b6cf045eef49ccdb/" />
  <meta name="manubot_pdf_url_versioned" content="https://phiresky.github.io/masters-thesis/v/83b6bb6e859b2ecf140a0ff1b6cf045eef49ccdb/manuscript.pdf" />
  <meta property="og:type" content="article" />
  <meta property="twitter:card" content="summary_large_image" />
  <link rel="icon" type="image/png" sizes="192x192" href="https://manubot.org/favicon-192x192.png" />
  <link rel="mask-icon" href="https://manubot.org/safari-pinned-tab.svg" color="#ad1457" />
  <meta name="theme-color" content="#ad1457" />
  <!-- end Manubot generated metadata -->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Bayesian and Attentive Aggregation for Cooperative Multi-Agent Deep Reinforcement Learning</h1>
</header>
<p><small><em>
This manuscript
(<a href="https://phiresky.github.io/masters-thesis/v/83b6bb6e859b2ecf140a0ff1b6cf045eef49ccdb/">permalink</a>)
was automatically generated
from <a href="https://github.com/phiresky/masters-thesis/tree/83b6bb6e859b2ecf140a0ff1b6cf045eef49ccdb">phiresky/masters-thesis@83b6bb6</a>
on July 22, 2021.
</em></small></p>
<h2 id="authors">Authors</h2>
<ul>
<li><strong>Robin Ruede</strong><br>
· <img src="images/github.svg" class="inline_icon" alt="GitHub icon" />
<a href="https://github.com/phiresky">phiresky</a><br>
<small>
Autonomous Learning Robots Lab, Karlsruhe Institute of Technology
</small></li>
</ul>
<h2 class="page_break_before" id="abstract">Abstract</h2>
<p>Multi-agent reinforcement learning (MARL) is an emerging field in reinforcement
learning with real world applications such as unmanned aerial vehicles,
search-and-rescue, and warehouse organization. There are many different
approaches for applying the methods used for single-agent reinforcement learning
to MARL. In this work, we survey different learning methods and environment
properties and then focus on a problem that persists through most variants: How
should one agent use the information gathered from a large and varying number of
observations of the world in order to make decisions? We focus on three
different methods for aggregating observations and compare them regarding their
training performance and sample efficiency. We introduce a policy architecture
for aggregation based on Bayesian conditioning and compare it to mean
aggregation and attentive aggregation used in related work. We show the
performance of the different methods on a set of cooperative tasks that can
scale to a large number of agents, including tasks that have other objects in
the world that need to be observed by the agents in order to solve the task.</p>
<p>We optimize the hyperparameters to be able to show which parameters lead to the
best results for each of the methods. In addition, we compare different variants
of Bayesian aggregation and compare the recently introduced Trust Region Layers
learning method to the commonly used Proximal Policy Optimization.</p>
<h2 id="introduction">Introduction</h2>
<p>Ant colonies, bee swarms, fish colonies, and migrating birds all exhibit
swarming behaviour to achieve a common goal or to gain an advantage over what’s
possible alone. Each individual within a swarm usually only has limited
perception, and often there is no central control that enforces a common goal or
gives instructions. Swarms of animals can self-organize and exhibt complex
emergent behaviour in spite of the limited intelligence, limited perception, and
limited strength of every individual.</p>
<p>In recent years and with the advent of deep reinforcement learning, it has
become possible to create artificial agents that solve fairly complex tasks both
in simulated environments as well as the real world, even when the path to the
goal is difficult to identify and the reward is sparse. Most of the research,
however, is focused on a single agent interacting with a world, and giving the
single agent as much power and flexibility as it needs to solve the task. Akin
to animal swarms, which often consist of fairly simple and limited individuals,
we focus on simple artificial agents that need to cooperate to solve a common
task, where the task is either difficult or impossible for an individual agent.
The agents thus need to be able to learn to work together and to act even with
the limited information they are able to perceive.</p>
<p>Swarms of artificial agents can have multiple applications, some inspired by
swarms in the animal kingdom, some going beyond. Robots can be used in logistics
to organize warehouses. Swarms of robots can be used to map out dangerous areas,
solve mazes, find trapped or injured people in search-and-rescue missions
quicker and more safely than humans
<span class="citation" data-cites="https://link.springer.com/article/10.1007/s43154-021-00048-3">[<a href="#ref-https://link.springer.com/article/10.1007/s43154-021-00048-3" role="doc-biblioref">Drew21</a>]</span>. Swarms of
walking robots could be used for animal shepherding
<span class="citation" data-cites="https://ieeexplore.ieee.org/abstract/document/9173524">[<a href="#ref-https://ieeexplore.ieee.org/abstract/document/9173524" role="doc-biblioref">HTKL21</a>]</span>. Drone swarms can be
used for entertainment in light shows
<span class="citation" data-cites="https://doi.org/10.3929/ethz-a-010831954">[<a href="#ref-https://doi.org/10.3929/ethz-a-010831954" role="doc-biblioref">WaKeAu21</a>]</span>, for advertising, for autonomous
surveillance <span class="citation" data-cites="https://doi.org/10.1117/12.830408">[<a href="#ref-https://doi.org/10.1117/12.830408" role="doc-biblioref">Bürk21</a>]</span>, or they could be weaponized
and used as military robots <span class="citation" data-cites="https://apps.dtic.mil/sti/citations/AD1039921">[<a href="#ref-https://apps.dtic.mil/sti/citations/AD1039921" role="doc-biblioref">Sand21</a>]</span>.
Further in the future, swarms of nanobots could be used in medicine to find and
target specific cells in the body
<span class="citation" data-cites="https://ieeexplore.ieee.org/document/7568316">[<a href="#ref-https://ieeexplore.ieee.org/document/7568316" role="doc-biblioref">CeSp21</a>]</span>. Swarms of autonomous
spacecraft can be used to explore space, to harvest resources, and to terraform
planets into paperclips
<span class="citation" data-cites="https://link.springer.com/article/10.1007/s00146-018-0845-5">[<a href="#ref-https://link.springer.com/article/10.1007/s00146-018-0845-5" role="doc-biblioref">TuDe21</a>]</span>.</p>
<p>Swarms of homogenous agents can be very resilient. Since they can be designed
such that no one individual is a critical component of the swarm as a whole,
individual failures do not necessarily result in a critical collapse of the
whole swarm.</p>
<p>Robots can be controlled by explicit algorithmic behaviour, but that can be
complicated, inflexible and fragile, as is shown by the success in recent
applications of reinforcmeent learning to control tasks that have not been
solved by pre-programmed algorithms <span class="citation" data-cites="https://arxiv.org/abs/1808.00177">[<a href="#ref-https://arxiv.org/abs/1808.00177" role="doc-biblioref">ABC21</a>]</span>.
Controlling robot swarms using policies learned with deep reinforcement learning
has promising results in recent literature such as <span class="citation" data-cites="https://arxiv.org/abs/1909.07528">[<a href="#ref-https://arxiv.org/abs/1909.07528" role="doc-biblioref">BKMW21</a>]</span>.</p>
<p>To apply deep reinforcement learning to multi-agent systems , we need to make a
few adjustments to the existing learning algorithms and figure out how best to
design the policy network. Specifically, we need a way to feed a large and
varying amount of observations from the neighboring agents into the fixed size
input of a dense neural network. In this work, we consider three aggregation
methods on a set of different multi-agent tasks: Mean aggregation, Bayesian
aggregation, and attentive aggregation. Our main goal is to compare these
methods with regards to their training performance and sample efficiency. We
limit ourselves to a specific subset of MARL tasks that are fully cooperative
with a team reward, with homogenous agents in a
centralized-learning/decentralized-execution setup.</p>
<p>We first give an overview over all the preliminaries we need for our work in
<a href="#sec:preliminaries">Section 1.4</a>, including reinforcement learning in general, multi-agent
reinforcement learning, and the background for the aggregation methods we use.
Next we describe related work in <a href="#sec:relatedwork">Section 1.5</a>. Then, we describe our contribution in <a href="#sec:contribution">Section 1.6</a> with details about the policy
architecture and the different aggregation methods. Our experimental setup,
including the specific environments we use to carry out our experiments are
described in <a href="#sec:experiments">Section 1.7</a>. Finally, we show and interpret the results of our
experiments in <a href="#sec:results">Section 1.8</a> and talk about the conclusions we can draw from the
experiments as well as the potential for future work in <a href="#sec:conclusion">Section 1.9</a>.</p>
<h2 id="sec:preliminaries">Preliminaries</h2>
<p>In this chapter, we introduce the preliminaries we need for our work. We
describe reinforcement learning and two specific policy gradient training
methods used for deep reinforcement learning in general and then the specifics
of RL for multi-agent systems. While introducing the different variants and
properties of multi-agent reinforcement learning we also describe the related
work.</p>
<h3 id="reinforcement-learning">Reinforcement Learning</h3>
<p>Reinforcement learning is a method of training an agent to solve a task in an
environment. As opposed to supervised learning, there is no explicit path to a
solution. Instead, the agent iteratively takes actions that affect its
environment, and receives a reward when specific conditions are met. The reward
can be dense (positive or negative signal at every step) or very sparse (only a
binary reward at the end of an episode).</p>
<p>Reinforcement learning problems are usually defined as Markov Decision Processes
(MDPs). An MDP is an extension of a Markov chain, which is a set of states with
a transition probability between each pair of states. The probability only
depends on the current state, not the previous states.</p>
<p>MDPs extend Markov chains, adding a set of actions (that allow an agent to
influence the transition probabilities) and rewards (that motivate the agent).
An MDP thus consists of:</p>
<ul>
<li>A set of states (the <em>state space</em>)</li>
<li>A set of actions (the <em>action space</em>)</li>
<li>The transition probability that a specific action in a specific state leads to
specific second state</li>
<li>The reward function that specifies the immediate reward an agent receives
depending on the previous state, the action, and the new state</li>
</ul>
<p>The MDP can either run for an infinite period or finish after a number of
transitions.</p>
<p>The method by which an agent chooses its actions based on the current state is
called a <em>policy</em>. The policy defines a probability for taking each action based
on a specific state.</p>
<p>Based on a policy we can also define the <em>value function</em>, which is the sum of
all future rewards that an agent gets based on an initial state and a specific
policy. The value function can also include an exponential decay for weighing
future rewards.</p>
<p>Often we need to extend MDPs to allow for an observation model: A Partially
observable Markov decision process (POMDP) is an extension to MDPs that
introduces an indirection between the set of states and the input to the policy
(called an <em>observation</em>). In the definition of a POMDP a set of observations is
added with a probability of each state leading to a specific observation. The
policy now depends on the observation and the action instead of the state and
the action.</p>
<p>To successfully solve a reinforcement learning task, we need to find a policy
that has a high expected reward - we want to find the <em>optimal policy function</em>
that has the highest value function on the initial states of our environment.
Since finding the optimal policy directly is impossible for even slightly
complicated tasks, we instead use optimization techniques.</p>
<h3 id="actor-critic-training-methods">Actor-Critic Training Methods</h3>
<p>Policy gradient training methods are a reinforcement learning technique that
optimize the parameters of a policy using gradient descent. Actor-critic methods
optimize both the policy and an estimation of the value function at the same
time.</p>
<p>There are multiple commonly used actor critic training methods such as Trust
Region Policy Optimization (TRPO)
<span class="citation" data-cites="http://proceedings.mlr.press/v37/schulman15.html">[<a href="#ref-http://proceedings.mlr.press/v37/schulman15.html" role="doc-biblioref">SLAJ21</a>]</span>, Proximal Policy
Optimization (PPO) <span class="citation" data-cites="https://arxiv.org/abs/1707.06347">[<a href="#ref-https://arxiv.org/abs/1707.06347" role="doc-biblioref">SWDR21</a>]</span> and Soft Actor Critic
<span class="citation" data-cites="https://arxiv.org/abs/1801.01290">[<a href="#ref-https://arxiv.org/abs/1801.01290" role="doc-biblioref">HZAL21</a>]</span>. We run most of our experiments with one
of the most commonly used methods (PPO) and also explore a new method that
promises stabler training (PG-TRL).</p>
<h4 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</h4>
<p>Proximal Policy Optimization <span class="citation" data-cites="https://arxiv.org/abs/1707.06347">[<a href="#ref-https://arxiv.org/abs/1707.06347" role="doc-biblioref">SWDR21</a>]</span> is a actor-critic policy gradient method for
reinforcement learning. In PPO, each training step consists of collecting a set
of trajectory rollouts, then optimizing a “surrogate” objective function using
stochastic gradient descent. The surrogate objective function approximates the
policy gradient while enforcing a trust region by clipping the update steps. PPO
is a successor to Trust Region Policy Optimization (TRPO) with a simpler
implementation and empirically better performance.</p>
<p>PPO optimizes the policy using</p>
<p><span class="math display">\[θ_{k+1} = \text{argmax}_{θ} E_{s,a \sim π_{θ_k}} [L(s, a, θ_k, θ)]\]</span></p>
<p>Where <span class="math inline">\(π_{θ_k}\)</span> is a policy with parameters <span class="math inline">\(θ\)</span> in training step <span class="math inline">\(k\)</span>, <span class="math inline">\(s\)</span> is the
state, <span class="math inline">\(a\sim π_{θ_k}\)</span> is the action distribution according to the the policy at
step <span class="math inline">\(k\)</span>. L is given by</p>
<p><span class="math display">\[L(s,a,θ_k,θ) = \min \left( \frac{π_θ(a|s)}{π_{θ_k}(a|s)} A^{π_{θ_k}}(s,a), \text{clip}(\frac{π_θ(a|s)}{π_{θ_k}(a|s)}, 1 - ε, 1 + ε) A^{π_{θ_k}}(s,a) \right).\]</span></p>
<p><span class="math inline">\(A\)</span> is the advantage of taking a specific action in a specific state as opposed
to the other actions as weighted by the current policy, estimated using
Generalized Advantage Estimation <span class="citation" data-cites="https://arxiv.org/abs/1506.02438">[<a href="#ref-https://arxiv.org/abs/1506.02438" role="doc-biblioref">SMLJ21</a>]</span> based on
the estimated value function.</p>
<p>Since the performance of PPO depends on a number of implementation details and
quirks, we use the stable-baselines3 <span class="citation" data-cites="https://github.com/DLR-RM/stable-baselines3">[<a href="#ref-https://github.com/DLR-RM/stable-baselines3" role="doc-biblioref">RHEG19</a>]</span> implementation of
PPO-Clip, which has been shown to perform as well as the original OpenAI
implementation
<span class="citation" data-cites="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html_x35_results">[<a href="#ref-https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html_x35_results" role="doc-biblioref">RHEG21</a>]</span>.</p>
<p>We use PPO as the default for most of our experiments since it is widely used in
other deep reinforcement learning work.</p>
<h4 id="sec:trl">Trust Region Layers (PG-TRL)</h4>
<p>Differentiable trust region layers are an alternative method to enforce a trust
region during policy updates introduced by <span class="citation" data-cites="https://openreview.net/forum?id_x61_qYZD-AO1Vn">[<a href="#ref-https://openreview.net/forum?id_x61_qYZD-AO1Vn" role="doc-biblioref">OBNZ21</a>]</span>. PPO uses a fixed clipping ratio
to enforce the trust region, which can result in unstable training. Instead of
using a fixed clipping boundary, in PG-TRL a surrogate policy is created with a
new layer at the end that projects the unclipped policy back into the trust
region. The trust region and the projection is based on either the KL-divergence
<span class="citation" data-cites="doi:10.1214/aoms/1177729694">[<a href="#ref-doi:10.1214/aoms/1177729694" role="doc-biblioref">KuLe21</a>]</span> or the Wasserstein <span class="math inline">\(W_2\)</span> distance
<span class="citation" data-cites="https://www.springer.com/gp/book/9783540710493">[<a href="#ref-https://www.springer.com/gp/book/9783540710493" role="doc-biblioref">Vill21</a>]</span>.</p>
<p>After each training step, the projected new policy depends on the previous
policy. To prevent an infinite stacking of old policies, the explicitly
projected policy is only used as a surrogate, while the real policy as based on
a learned approximation. To prevent the real policy and the projected policy
from diverging, the divergence between them is computed again in the form of the
KL divergence or the Wasserstein <span class="math inline">\(W_2\)</span>. The computed divergence (<em>trust region
regression loss</em>) is then added to the policy gradient loss function with a high
factor.</p>
<p>We explore PG-TRL as an alternative training method to PPO.</p>
<h3 id="multi-agent-reinforcement-learning-marl">Multi-Agent Reinforcement Learning (MARL)</h3>
<p>Usually, reinforcement learning algorithms operate on POMDPs. POMDPs only have a
single agent interacting with the environment, so for multi-agent settings, we
need a different system that can model multiple agents interacting with the
world. There’s different ways of extending POMDPs to work for multi-agent tasks. Decentralized POMDPs (Dec-POMDP)
<span class="citation" data-cites="https://www.springer.com/gp/book/9783319289274">[<a href="#ref-https://www.springer.com/gp/book/9783319289274" role="doc-biblioref">OlAm21</a>]</span> are a generalization of
POMDPs that split the action and observation spaces</p>
<ul>
<li>A set of <span class="math inline">\(n\)</span> agents <span class="math inline">\(D={1,\dots,n}\)</span></li>
<li>A set of states <span class="math inline">\(S\)</span></li>
<li>A set of joint actions <span class="math inline">\(A\)</span></li>
<li>A set of transition probabilities between states <span class="math inline">\(T(s,a,s&#39;) = P(s&#39;|s,a)\)</span></li>
<li>A set of joint observations <span class="math inline">\(O\)</span></li>
<li>An immediate reward function <span class="math inline">\(R: S × A → \mathbb{R}\)</span></li>
</ul>
<p>Note that this definition is very similar to a normal POMDP. The difference is
that the set of joint observations and joint actions consists of a tuple of the
observations/actions of each individual agent
(i.e. <span class="math inline">\(a\in A = \langle a_i,\dots,a_n \rangle\)</span>). In Dec-POMDPs every agent can have
differing observations and actions.</p>
<h3 id="environment-model-and-learning-process">Environment Model and Learning Process</h3>
<p>In our experiments, we impose a set of restrictions on the environments and
learning process. The restrictions we impose here are mostly based on
<span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span>. Here, we describe the major differing factors of both the learning
process and the environments, as well as the variants we choose to consider.</p>
<p>In general, the agents in a multi-agent environments can differ in their
intrinsic properties. For example, they can have different control dynamics,
maximum speeds, different observation systems, or different possible actions. We
only consider environments with homogenous agents: All agents have the same
physical properties, observation space, and action space. They only differ in
their extrinsic properties, e.g., their current position, rotation, and speed.
This also causes them to have a different perspective, different observations
and thus different actions, resulting in differing behavior even when they are
acting according to the same policy.</p>
<p>We thus only consider a subset of Dec-POMDPs, namely those where the agents are
homogenous - each agent has the same possible observations and actions. This has
also been described as a SwarMDP by
<span class="citation" data-cites="https://dl.acm.org/doi/10.5555/3091125.3091320">[<a href="#ref-https://dl.acm.org/doi/10.5555/3091125.3091320" role="doc-biblioref">ŠKZK21</a>]</span>.</p>
<p>We only consider cooperative environments, and we use the same reward function
for all agents. Real-world multi-agent tasks are usually cooperative since in
adversarial environments, one entity would not have control over multiple
adversarial parties.</p>
<p>We focus on global visibility since the additional noise introduced by local
observability would be detrimental to the quality of our results.</p>
<p>For training we use the centralized-learning/decentralized-execution (CLDE)
approach - a shared common policy is learned for all agents, but the policy is
executed by each agent separately.</p>
<p>PPO and other policy gradient methods are designed for single-agent
environments. We adapt PPO to the multi-agent setting without making any major
changes: The policy parameters are shared for all agents, and each agent gets
the same global reward. The value function for advantage estimation is based on
the same architecture as the policy. Since the stable-baselines3 implementation
of PPO is already written for vectorized environments (collecting trajectories
from many environments running in parallel), we create a new VecEnv
implementation that flattens multiple agents in multiple environments.</p>
<p>Similarily to the setup used for TRPO by <span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span>, we collect the data of each
agent as if that agent was the only agent in the world. For example, a batch of
a single step of 10 agents each in 20 different environment becomes 200 separate
training samples. Each agent still only has access to its own local
observations, not the global system state. This means that during inference
time, each agent has to act independently, based on the observations it makes
locally.</p>
<h3 id="aggregation-methods">Aggregation Methods</h3>
<p>The observations of each agent in a MARL task contains a varying number of
observables. The observables can be clustered into groups where each observable
is of the same kind and shape. For example, one observable group would contain
all the neighboring agents, while another would contain, e.g., a specific type
of other observed objects in the world.</p>
<p>We need a method to aggregate the observations in one observable group into a
format that can be used as the input of a neural network. Specifically, this
means that the output of the aggregation method needs to have a fixed
dimensionality. In the following, we present different aggregation methods and
their properties.</p>
<h4 id="concatenation">Concatenation</h4>
<p>The simplest aggregation method is concatenation, where each observable is
concatenated along the feature dimension into a single observation vector. This
method has a few issues however: We can have a varying number of observables,
but since the input size of the neural network is fixed, we need to set a
maximum number of observables, and set the input dimensionality of the policy
architecture proportional to that maximum. Concatenation also ignores the other
useful properties of the observables, namely the uniformity (the feature at
index <span class="math inline">\(i\)</span> of one observable has the same meaning as the feature at index <span class="math inline">\(i\)</span> of
every other observable in a group) and the exchangeability (the order of the
observables is either meaningless or variable).</p>
<p>Concatenation scales poorly with a large number of observables since the input
size of the neural network has to scale proportionally to the maximum number of
observables.</p>
<p>Concatenation is used for example by <span class="citation" data-cites="https://dl.acm.org/doi/10.5555/3295222.3295385">[<a href="#ref-https://dl.acm.org/doi/10.5555/3295222.3295385" role="doc-biblioref">LWTH21</a>]</span> to aggregate the neighboring agents’
observations and actions.</p>
<h4 id="mean-aggregation">Mean Aggregation</h4>
<p>Instead of concatenating each element <span class="math inline">\(o_i\)</span> in an observable group <span class="math inline">\(O\)</span>, we can
also interpret each element as a sample of a distribution that describes the
current system state. We use the empirical mean of the samples to retrieve a
representation of the system state <span class="math inline">\(ψ_O\)</span> based on all observed observables, as
shown in <a href="#eq:meanagg">Equation 1</a>.</p>
<p><span id="eq:meanagg"><span class="math display">\[ψ_O = μ_O = \frac{1}{|O|} \sum_{o_i ∈ O} o_i\qquad(1)\]</span></span></p>
<p>The encoder is an arbitrary function that maps the observation into a latent
space, and can be represented by a neural network with shared weights across the
observables in a observable group. <span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span> used mean aggregation for deep
multi-agent reinforcement learning and compared it to other aggregation methods.
<span class="citation" data-cites="https://www.semanticscholar.org/paper/Using-M-Embeddings-to-Learn-Control-Strategies-for-Gebhardt-H_x37_C3_x37_BCttenrauch/9f550815f8858e7c4c8aef23665fa5817884f1b3">[<a href="#ref-https://www.semanticscholar.org/paper/Using-M-Embeddings-to-Learn-Control-Strategies-for-Gebhardt-H_x37_C3_x37_BCttenrauch/9f550815f8858e7c4c8aef23665fa5817884f1b3" role="doc-biblioref">GeHüNe21</a>]</span> applied mean aggregation to more complex tasks in more realistic
simulated environments.</p>
<p>Mean aggregation is strongly related to mean field theory. Mean field theory is
a general principle of modeling the effect that a large number of particles have
by averaging them into a single field, ignoring the individual variances of each
particle. The application of mean field theory for multi-agent systems were
formally defined by
<span class="citation" data-cites="https://link.springer.com/article/10.1007/s11537-007-0657-8">[<a href="#ref-https://link.springer.com/article/10.1007/s11537-007-0657-8" role="doc-biblioref">LaLi21</a>]</span> as <em>Mean Field
Games</em>. In MARL, mean field Q-learning and mean field actor-critic was defined
and evaluated by <span class="citation" data-cites="http://proceedings.mlr.press/v80/yang18d.html">[<a href="#ref-http://proceedings.mlr.press/v80/yang18d.html" role="doc-biblioref">YLLZ21</a>]</span>. <span class="citation" data-cites="https://ieeexplore.ieee.org/abstract/document/9137257">[<a href="#ref-https://ieeexplore.ieee.org/abstract/document/9137257" role="doc-biblioref">CQZW21</a>]</span>
use mean fields productively for control of a large number of unmanned aerial
vehicles.</p>
<h4 id="aggregation-with-other-pooling-functions">Aggregation With Other Pooling Functions</h4>
<p>Instead of using the empirical mean, other aggregation methods can also be used:</p>
<p>Max pooling:</p>
<p><span class="math display">\[ψ_O = \max_{o_i ∈ O} o_i\]</span></p>
<p>Softmax pooling:</p>
<p><span class="math display">\[ψ_O = \sum_{o_i ∈ O} o_i \frac{e^{o_i}}{\sum_{o_j ∈ O} e^{o_j}}\]</span></p>
<p>Max-pooling is widely used in convolutional neural networks to reduce the image
dimensionality where it consistently outperforms mean (average) pooling. Softmax
aggregation was used by <span class="citation" data-cites="https://arxiv.org/abs/1703.04908">[<a href="#ref-https://arxiv.org/abs/1703.04908" role="doc-biblioref">MoAb21</a>]</span> for MARL.</p>
<h4 id="sec:bayesianagg1">Bayesian Aggregation</h4>
<p>Aggregation with Gaussian conditioning works by starting from a Gaussian prior
distribution and updating it using a probabilistic observation model for every
seen observation. Gaussian conditioning is widely used in applications such as
Gaussian process regression
<span class="citation" data-cites="https://link.springer.com/chapter/10.1007/978-3-540-28650-9_4">[<a href="#ref-https://link.springer.com/chapter/10.1007/978-3-540-28650-9_4" role="doc-biblioref">Rasm21</a>]</span>.</p>
<p>We consider Bayesian aggregation as defined by <span class="citation" data-cites="https://openreview.net/forum?id_x61_ufZN2-aehFa">[<a href="#ref-https://openreview.net/forum?id_x61_ufZN2-aehFa" role="doc-biblioref">VFGD21</a>, sec
7.1]</span>. We introduce <span class="math inline">\(z\)</span> as a random variable with a Gaussian distribution:</p>
<p><span class="math display">\[z \sim \mathcal{N}(μ_z,σ_z^2)\quad (p(z) ≡ \mathcal{N}(μ_z,σ_z^2))\]</span></p>
<p>Initially, this random variable is estimated using a diagonal Gaussian prior as
an a-priori estimate:</p>
<p><span class="math display">\[p_0(z)≡\mathcal{N}(μ_{z_0}, diag(σ_{z_0}^2))\]</span></p>
<p>This prior is then updated with Bayesian conditioning using each of the observed
elements <span class="math inline">\(r_i\)</span>. We interpret each observation as a new sample from the
distribution <span class="math inline">\(p(z)\)</span>, each with a mean <span class="math inline">\(r_i\)</span> and a standard deviation <span class="math inline">\(σ_{r_i}\)</span>.
We use the probabilistic observation model and consider the conditional
probability <span class="math display">\[p(r_i|z) ≡ \mathcal{N}(r_n|z, σ_{r_i}^2).\]</span></p>
<p>With standard Gaussian conditioning
<span class="citation" data-cites="https://www.springer.com/gp/book/9780387310732">[<a href="#ref-https://www.springer.com/gp/book/9780387310732" role="doc-biblioref">Bish21</a>]</span> this leads to the closed
form factorized posterior description of <span class="math inline">\(z\)</span>:</p>
<p><span class="math display">\[\sigma_z^2 = \frac{1}{\frac{1}{\sigma_{z_0}^2} + \sum_{i=1}^n{\frac{1}{\sigma_{r_i}^2}}}\]</span></p>
<p><span class="math display">\[\mu_z = \mu_{z_0} + \sigma_z^2 \cdot \sum_{i=1}^{n}{\frac{(r_i-\mu_{z_0})}{\sigma_{r_i}^2}}\]</span></p>
<p>Bayesian aggregation was used by <span class="citation" data-cites="https://openreview.net/forum?id_x61_ufZN2-aehFa">[<a href="#ref-https://openreview.net/forum?id_x61_ufZN2-aehFa" role="doc-biblioref">VFGD21</a>]</span> for context
aggregation in conditional latent variable (CLV) models. There is no related
work using Bayesian aggregation for MARL.</p>
<h4 id="sec:mha">Attention Mechanisms</h4>
<p>Attention mechanisms are commonly used in other areas of machine learning.</p>
<p>An attention function computes some compatibility between a <em>query</em> <span class="math inline">\(Q\)</span> and a <em>key</em> <span class="math inline">\(K\)</span>, and uses this compatibility as the weight to compute a weighted sum of the <em>values</em> <span class="math inline">\(V\)</span>. The query, key, and value are all vectors.</p>
<!-- For attentive aggregation, the observations from the different agents are first
attended to by some attention mechanism, then the resulting new feature vector
is aggregated using some aggregation mechanism. -->
<!-- {attention: 30,31,32,33 (from ARE paper)} -->
<p>The
multi-head attention mechanism was introduced by <span class="citation" data-cites="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">[<a href="#ref-https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" role="doc-biblioref">VSPU21</a>]</span>
and is the core of the state-of-the-art model for many natural language
processing tasks.</p>
<p>We only consider the specific multi-head residual masked self attention variant
of attention mechanisms for observation aggregation used by <span class="citation" data-cites="https://arxiv.org/abs/1909.07528">[<a href="#ref-https://arxiv.org/abs/1909.07528" role="doc-biblioref">BKMW21</a>]</span>.</p>
<p>The attention function is a scaled dot-product attention as described by <span class="citation" data-cites="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">[<a href="#ref-https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" role="doc-biblioref">VSPU21</a>]</span>:</p>
<p><span class="math display">\[\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\]</span></p>
<p><span class="math inline">\(d_k\)</span> is the dimensionality of the key <span class="math inline">\(K\)</span>.</p>
<p>Instead of using only a single attention function, <span class="citation" data-cites="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">[<a href="#ref-https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" role="doc-biblioref">VSPU21</a>]</span> uses multiple independent attention heads. The inputs (<span class="math inline">\(Q, K, V\)</span>) of each of the heads <span class="math inline">\(1,\dots,n\)</span> as well as the concatenated output is transformed with a separately learned dense layers (described as weight matrices <span class="math inline">\(W_i^Q, W_i^K, W_i^V, W^O\)</span>). The full multi-head attention module <span class="math inline">\(MHA()\)</span> thus looks like this:</p>
<p><span class="math display">\[\text{head}_i = \text{Attention}(QW_i^Q,KW_i^K,VW_i^V)\]</span></p>
<p><span class="math display">\[\text{MHA}(Q,K,V)=\text{concat}(\text{head}_1,\dots,\text{head}_n)W^O\]</span></p>
<p>For self-attention, the query <span class="math inline">\(Q\)</span>, the key <span class="math inline">\(K\)</span> and the value <span class="math inline">\(V\)</span> are all set to
the same input value. In <span class="citation" data-cites="https://arxiv.org/abs/1909.07528">[<a href="#ref-https://arxiv.org/abs/1909.07528" role="doc-biblioref">BKMW21</a>]</span>, the authors combine the
multi-head attention with a mean aggregation. Note that they only use the
attention mechanism to individually transform the information from each separate
observable into a new feature set instead of directly using it as a weighing function for
the aggregation.</p>
<p><span class="citation" data-cites="http://proceedings.mlr.press/v97/iqbal19a.html">[<a href="#ref-http://proceedings.mlr.press/v97/iqbal19a.html" role="doc-biblioref">IqSh21</a>]</span> use a different approach with
a decentralized policy and a centralized critic. An attention mechanism is used
to weigh the features from the other agents, then aggregates them to calculate
the value function.</p>
<p><span class="citation" data-cites="https://ieeexplore.ieee.org/document/9049415">[<a href="#ref-https://ieeexplore.ieee.org/document/9049415" role="doc-biblioref">LiTa21</a>]</span> first encode all aggregatable observations together with the proprioceptive
observations with an “extrinsic encoder”, then compute attention weights using
another encoder from that and aggregate the features retrieved from a separate
“intrinsic encoder” using those weights.</p>
<h4 id="other-aggregation-methods">Other Aggregation Methods</h4>
<p><span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span> also did experiments with aggregation into histograms and aggregation
with radial basis functions, though the results indicated they were outperformed
by a neural network encoder with mean aggregation.</p>
<!-- @tocommunicate and
- based on temporal information (recurrent nns)
-

-->
<h2 id="sec:relatedwork">Related Work</h2>
<p>There are many variants of applying reinforcement learning to multi-agent
systems.</p>
<p>An overview over recent MARL work and some of the differing properties can be
found in <span class="citation" data-cites="https://arxiv.org/abs/1911.10635">[<a href="#ref-https://arxiv.org/abs/1911.10635" role="doc-biblioref">ZhYaBa21</a>]</span> and
<span class="citation" data-cites="https://www.mdpi.com/2076-3417/11/11/4948">[<a href="#ref-https://www.mdpi.com/2076-3417/11/11/4948" role="doc-biblioref">CCDF21</a>]</span>.</p>
<!-- [@tocommunicate]:
  https://proceedings.neurips.cc/paper/2016/hash/c7635bfd99248a2cdef8249ef7bfbef4-Abstract.html

- Learning to Communicate with Deep Multi-Agent Reinforcement Learning
  [@tocommunicate] -->
<!-- - Multi-agent Reinforcement Learning as a Rehearsal for Decentralized Planning
  https://www.sciencedirect.com/science/article/abs/pii/S0925231216000783 -->
<h3 id="centralized-vs-decentralized-learning">Centralized vs Decentralized Learning</h3>
<p>During training, the agent policies car either be learned in a centralized or a
decentralized fashion.</p>
<p>In decentralized learning, each agent learns their own policy, while in
centralized learning, the policy is shared between all agents. Decentralized
learning has the advantage that the agents do not need to be homogenous, since
the learned policy can be completely different. It also means that it’s harder
for the agents to learn to collaborate though, since they are not intrinsically
similar to each other. Decentralized learning is used in
<span class="citation" data-cites="https://arxiv.org/abs/1806.00877 https://ieeexplore.ieee.org/document/6415291 http://proceedings.mlr.press/v80/zhang18n.html">[<a href="#ref-https://ieeexplore.ieee.org/document/6415291" role="doc-biblioref">KaMoPo13</a>, <a href="#ref-https://arxiv.org/abs/1806.00877" role="doc-biblioref">WYWH21</a>, <a href="#ref-http://proceedings.mlr.press/v80/zhang18n.html" role="doc-biblioref">ZYLZ21</a>]</span>.</p>
<p>Centralized learning means the training process happens centralized, with a
single policy. It has the advantage of only needing to train one policy network
and the possibility of being more sample-efficient. Centralized learning
requires homogenous agents since the policy network parameters are shared across
all agents. It is used for example in <span class="citation" data-cites="https://arxiv.org/abs/1705.08926 https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5">[<a href="#ref-https://arxiv.org/abs/1705.08926" role="doc-biblioref">FFAN21</a>, <a href="#ref-https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5" role="doc-biblioref">GuEgKo21</a>]</span>.</p>
<h3 id="centralized-vs-decentralized-execution">Centralized vs Decentralized Execution</h3>
<p>When using centralized learning, it is possible to use a single policy to output
actions for all agents at the same time. This is called “centralized execution”.
The policy chooses the actions based on the global system state in a “birds-eye”
view of the world. The disadvantage is that this means agents can’t act
independently if the environment has restrictions such as partial observability,
and it is less robust to communication failures and to agent “deaths”.</p>
<p>The alternative is decentralized execution - the observation inputs and action
outputs of the policy networks are local to a single agent. This can formally be
described as a <em>Decentralized Partially Observable Markov Decision Process</em>
(Dec-POMDP) <span class="citation" data-cites="https://link.springer.com/chapter/10.1007/978-3-642-27645-3_15">[<a href="#ref-https://link.springer.com/chapter/10.1007/978-3-642-27645-3_15" role="doc-biblioref">Olie21</a>]</span>.
When combining centralized learning with decentralized execution, there is only
one policy network that is shared between all agents but the inputs and outputs
of the policy are separate.</p>
<p>Centralized-learning with decentralized-execution is thus a compromise between
performance, robustness, and sample efficiency. CLDE is used e.g. by <span class="citation" data-cites="https://dl.acm.org/doi/10.5555/3295222.3295385 https://jmlr.org/beta/papers/v20/18-476.html https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5 https://arxiv.org/abs/1705.08926 https://proceedings.neurips.cc/paper/2016/hash/c7635bfd99248a2cdef8249ef7bfbef4-Abstract.html">[<a href="#ref-https://proceedings.neurips.cc/paper/2016/hash/c7635bfd99248a2cdef8249ef7bfbef4-Abstract.html" role="doc-biblioref">FAFW21</a>, <a href="#ref-https://arxiv.org/abs/1705.08926" role="doc-biblioref">FFAN21</a>, <a href="#ref-https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5" role="doc-biblioref">GuEgKo21</a>, <a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>, <a href="#ref-https://dl.acm.org/doi/10.5555/3295222.3295385" role="doc-biblioref">LWTH21</a>]</span>.</p>
<h3 id="cooperative-adversarial-team-based">Cooperative, Adversarial, Team-Based</h3>
<p>Multi-agent environments can be cooperative, adversarial, or team-based.
Cooperative environments are those where all the agents have one common goal. In
adversarial tasks each agent has their own independent goal that conflicts with
the goal of the other agents. An example for a cooperative environment is the
rendezvous task: All agents need to meet up at a single point, where the agents
have to decide independently on the location. The reward here is the negative
average pairwise distance of the agents, see <a href="#sec:rendezvous">Section 1.7.2.1</a>. Note that a
cooperative environments can also include other adversarial agents, as long as
those agents are controlled by an explicit algorithm and not a learned policy.
From the perspective of the policy, these agents are considered part of the
environment.</p>
<p>Cooperative learning can be done with separate agent rewards or a single common
reward. Cooperative learning with a single reward means the reward must be
somewhat sparse, since each agent cannot easily judge what the impact of it’s
own actions were on the reward in any given time step. This also makes it
impossible for an agent to gain an egoistic advantage over the other agents,
enforcing the learned policy to become Pareto optimal. Another approach was
introduced by <span class="citation" data-cites="https://ieeexplore.ieee.org/document/4399095">[<a href="#ref-https://ieeexplore.ieee.org/document/4399095" role="doc-biblioref">MaLaLe21</a>]</span>, who create a
compromise between the sample-efficiency of individual rewards and the
Pareto-optimality of a common reward for cooperative MARL settings with their
<em>Hysteretic Q-Learning</em> algorithm that jointly optimizes both an individual as
well as a common reward.</p>
<p>Adversarial environments are usually zero-sum, that is the average reward over
all agents is zero. An example for an adversarial environment is the <em>Gather</em>
task described by <span class="citation" data-cites="https://ojs.aaai.org/index.php/AAAI/article/view/11371">[<a href="#ref-https://ojs.aaai.org/index.php/AAAI/article/view/11371" role="doc-biblioref">ZYCZ21</a>]</span>: In
a world with limited food, agents need to gather food to survive. They can also
kill each other to reduce the scarcity of the food. Another example of an
adversarial task is Go <span class="citation" data-cites="https://www.nature.com/articles/nature24270">[<a href="#ref-https://www.nature.com/articles/nature24270" role="doc-biblioref">SSSA21</a>]</span> as well
as many other board games, though these usually have a fairly small number of
agents.</p>
<p>Team-based tasks have multiple teams that each have a conflicting goal, but from
the perspective of each team the task is cooperative (with a single reward
function). The team reward can either be defined directly for the team or by
averaging a reward function of each team member.</p>
<!-- One example of a team-based environment is [@{https://openai.com/blog/openai-five/}]. -->
<p>An example of a team-based environment is the OpenAI Hide-and-Seek Task
<span class="citation" data-cites="https://arxiv.org/abs/1909.07528">[<a href="#ref-https://arxiv.org/abs/1909.07528" role="doc-biblioref">BKMW21</a>]</span>. In this task, there are two teams of agents in a simulated physical
world with obstacles, and members of one team try to find the members of the
other team. The Hide-team is rewarded +1 if none of the team members is seen by
any seeker, and -1 otherwise. The Seek-team is given the opposite reward.</p>
<h3 id="partial-visibility">Partial Visibility</h3>
<p>The observations that each agent receives in our experiments are local. For
example, if one agent sees another, that agent’s properties are observed
relative to the current agent - the distance, relative bearing, and relative
speed.</p>
<p>In addition, each agent may only have local visibility, for example it can only
observe the positions of agents and objects in the world within some radius or
the visibility can be hindered by obstacles. In <span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span> both the local
and global visibility variants of the same tasks were considered.</p>
<h3 id="simultaneous-vs-turn-based">Simultaneous vs Turn-Based</h3>
<p>In general, multi-agent environments can be turn-based or simultaneous. In
turn-based environments each agent acts in sequence, with the world state
changing after each turn. In simultaneous environments, all agents act “at the
same time”, i.e. the environment only changes after all agents have acted in one
time step.</p>
<p>Simultaneous environments can be considered a subset of turn-based environments,
since a simultaneous environment can be converted to a turn-based one by fixing
the environment during turns and only applying the actions of each agent after
the time step is finished. For this reason the API of PettingZoo <span class="citation" data-cites="https://arxiv.org/abs/2009.14471">[<a href="#ref-https://arxiv.org/abs/2009.14471" role="doc-biblioref">TBGJ21</a>]</span>
is based around turn-based environments. Examples of turn-based environments
include most board games and many video games. Most real world scenarios are
simultaneous though, so we only consider environments with simultaneous actions.</p>
<h3 id="marl-tasks-in-related-work">MARL Tasks in Related Work</h3>
<p><span class="citation" data-cites="https://arxiv.org/abs/1909.07528">[<a href="#ref-https://arxiv.org/abs/1909.07528" role="doc-biblioref">BKMW21</a>]</span> create a team-based multi-agent environment with one team of agents
trying to find and “catch” the agents of the other team.</p>
<p><span class="citation" data-cites="https://ieeexplore.ieee.org/document/9049415">[<a href="#ref-https://ieeexplore.ieee.org/document/9049415" role="doc-biblioref">LiTa21</a>]</span> test their attention-based architecture on three environments: (1) A
catching game in a discrete 2D world, where multiple paddles moving in one
dimension try to catch multiple balls that fall down the top of the screen. (2)
A spreading game, where N agents try to cover N landmarks. The world is
continous, but the action space is discrete. This game is similar to the
spreading game defined in <span class="citation" data-cites="https://dl.acm.org/doi/10.5555/3295222.3295385">[<a href="#ref-https://dl.acm.org/doi/10.5555/3295222.3295385" role="doc-biblioref">LWTH21</a>]</span>. (3) StarCraft micromanagement: Two groups of
units in the StarCraft game battle each other, each unit controlled by an agent.</p>
<p><span class="citation" data-cites="https://arxiv.org/abs/2009.14471">[<a href="#ref-https://arxiv.org/abs/2009.14471" role="doc-biblioref">TBGJ21</a>]</span> create an infrastructure for multi-task environments similar to the
OpenAI Gym and add standardized wrappers for Atari multiplayer games, as well as
a reimplementation of the MAgent environments
<span class="citation" data-cites="https://ojs.aaai.org/index.php/AAAI/article/view/11371">[<a href="#ref-https://ojs.aaai.org/index.php/AAAI/article/view/11371" role="doc-biblioref">ZYCZ21</a>]</span>, the
Multi-Particle-Environments <span class="citation" data-cites="https://dl.acm.org/doi/10.5555/3295222.3295385">[<a href="#ref-https://dl.acm.org/doi/10.5555/3295222.3295385" role="doc-biblioref">LWTH21</a>]</span> and the SISL environments
<span class="citation" data-cites="https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5">[<a href="#ref-https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5" role="doc-biblioref">GuEgKo21</a>]</span>.</p>
<p><span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span> test mean aggregation on two environments: (1) A task where all agents
need to meet up in one point (rendezvous task). (2) A task where the agents try
to catch one or more evaders (pursuit task). We compare our method on both of
these tasks.</p>
<p><span class="citation" data-cites="https://www.semanticscholar.org/paper/Using-M-Embeddings-to-Learn-Control-Strategies-for-Gebhardt-H_x37_C3_x37_BCttenrauch/9f550815f8858e7c4c8aef23665fa5817884f1b3">[<a href="#ref-https://www.semanticscholar.org/paper/Using-M-Embeddings-to-Learn-Control-Strategies-for-Gebhardt-H_x37_C3_x37_BCttenrauch/9f550815f8858e7c4c8aef23665fa5817884f1b3" role="doc-biblioref">GeHüNe21</a>]</span> define a set of environments based on Kilobots, simple small round
robots in a virtual environment together with boxes. The Kilobots are tasked
with moving the boxes around or arranging them based on their color. We compare
our method on both the box assembly as well as the box clustering task.</p>
<h2 id="sec:contribution">Scalable Information Aggregation for Deep MARL Policies</h2>
<p>We introduce a policy architecture for deep reinforcement learning that projects
observations from one or more different kinds of observables into samples of
latent spaces, then aggregates them into a single latent value. This makes the
architecture scale to any number as well as a varying number of observables.</p>
<h3 id="policy-architecture">Policy Architecture</h3>
<p>In general, our policy architecture is based on <span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span> and <span class="citation" data-cites="https://arxiv.org/abs/1909.07528">[<a href="#ref-https://arxiv.org/abs/1909.07528" role="doc-biblioref">BKMW21</a>]</span>.</p>
<p>While the policy architecture and weights are shared for all agents, the inputs
are dependent on the current agent <span class="math inline">\(a_k\)</span>, thus leading to different outputs for
the action and value function for each agent.</p>
<p>Depending on the task, the inputs differ. In general, we always have the
proprioceptive observations (how the agent <span class="math inline">\(a_k\)</span> sees itself), and the
observations from the <span class="math inline">\(n\)</span> neighboring agents <span class="math inline">\(a_1,…,a_n\)</span>. The observations from
each neighbor have the same shape, but differ from the self-observations since
they can include other information like the distance between <span class="math inline">\(a_k\)</span> and <span class="math inline">\(a_i\)</span>,
and they may be missing some information that may not be known to <span class="math inline">\(a_k\)</span>. In
addition, there can be additional sets of observations, for example for objects
or scripted agents in the environment. The architecture can handle any amount of
observation sets, but the observations in each set must have the same shape.</p>
<p>We call each of the sets of observations “aggregation groups”.</p>
<p>First, we collect the observations for each instance <span class="math inline">\(1&lt;i&lt;n\)</span> of each aggregation
group <span class="math inline">\(G\)</span>. These observations can be relative to the current agent. Each
observation in the aggregation group <span class="math inline">\(G\)</span> is thus a function of the agent <span class="math inline">\(a_k\)</span>
and the instance <span class="math inline">\(g_i\)</span>:</p>
<p><span class="math display">\[o_{k→g_i} = \text{observe}(a_k, g_i)\]</span></p>
<p>After collecting the observations for aggregation group <span class="math inline">\(g\)</span>, the observations
are each encoded separately into a latent embedding space:</p>
<p><span class="math display">\[e_{k→g_i} = \text{enc}(o_{k→g_i})\]</span></p>
<p>The Encoder <span class="math inline">\(\text{enc()}\)</span> is a dense neural network with zero or more hidden
layers.</p>
<p>After being encoded, we interpret each instance of an aggregation group as one
sample of a latent space that represents some form of the information of the
aggregation group that is relevant to the agent. These samples are then
aggregated using one of the aggregation methods described below to get a single
latent space value for each aggregation group:</p>
<p><span class="math display">\[e_{k→G} = \text{agg}_{i=0}^n(e_{k→g_i})\]</span></p>
<p>We then concatenate all of the latent spaces as well as the proprioceptive
observations <span class="math inline">\(p\)</span> to get a single encoded value <span class="math inline">\(e_k\)</span>:</p>
<p><span class="math display">\[e_k = (p, G_1, G_2, ...)\]</span></p>
<p>This value is then passed through a decoder that consists of one or more dense
layers. Finally, the decoded value is transformed to the dimensionality of the
action space or to <span class="math inline">\(1\)</span> to get the output of the value function. While we share
the architecture for the policy and value function, we use a separate copy of
the compute graph for the value function, so the weights and training are
completely independent.</p>
<p><a href="#fig:model">Figure 1</a> shows a schematic of the general model architecture described above.</p>
<figure>
<img src="images/model.drawio.svg" id="fig:model" alt="Figure 1: A schematic of our general model architecture for deep MARL policies with scalable information aggregation. The observation inputs consist of the agent itself and multiple aggregatable observation groups. The observations from the aggregation groups are each passed though an encoder and aggregated with an aggregator. Afterwards all the aggregated observations are concatenated and decoded to get the policy and value function." /><figcaption aria-hidden="true">Figure 1: A schematic of our general model architecture for deep MARL policies with scalable information aggregation. The observation inputs consist of the agent itself and multiple aggregatable observation groups. The observations from the aggregation groups are each passed though an encoder and aggregated with an aggregator. Afterwards all the aggregated observations are concatenated and decoded to get the policy and value function.</figcaption>
</figure>
<p>The tasks we consider have a continuous action space. We use a diagonalized
Gaussian distribution where the mean <span class="math inline">\(μ\)</span> of each action is output by the neural
network while the variance of each action is a free-standing learnable variable
only passed through <span class="math inline">\(\exp\)</span> or <span class="math inline">\(\text{softplus}\)</span> to ensure positivity.</p>
<h4 id="meanmaxsoftmax-aggregation">Mean/Max/Softmax Aggregation</h4>
<p>Each sample in the latent space is weighted by a function <span class="math inline">\(\text{weigh}()\)</span> and
aggregated using an aggregation operator <span class="math inline">\(\bigoplus\)</span>:</p>
<p><span class="math display">\[e_{k→G} = \bigoplus_{i=1}^n \text{weigh}(e_{k→g_i})\]</span></p>
<p>For mean aggregation, the weighing function multiplies by <span class="math inline">\(\frac{1}{n}\)</span> and the
aggregation operator is the sum:</p>
<p><span class="math display">\[e_{k→G} = \sum_{i=1}^n \frac{1}{n} \cdot (e_{k→g_i})\]</span></p>
<p>For max aggregation, the weight is <span class="math inline">\(1\)</span> and the aggregation operator takes the
largest value:</p>
<p><span class="math display">\[e_{k→G} = \max_{i=1}^n e_{k→g_i}\]</span></p>
<p>For softmax aggregation, the weight is based on the softmax function and the
aggregation operator is the sum:</p>
<p><span class="math display">\[e_{k→G} = \sum_{i=1}^n \left(\frac{\exp(e_{k→g_i})}{\sum_{j=1}^n \exp(e_{k→g_j})}\right) e_{k→g_i}\]</span></p>
<h4 id="sec:bayesianagg">Bayesian Aggregation</h4>
<p>We use a separate latent space and thus a separate observation model <span class="math inline">\(p(z)\)</span> for
each aggregation group.</p>
<p>To make the Bayesian aggregation as described in <a href="#sec:bayesianagg1">Section 1.4.5.4</a> work in our
policy network, we need an estimation describing the Gaussian prior (<span class="math inline">\(μ_{z_0}\)</span>
and <span class="math inline">\(σ_{z_0}^2\)</span>) as well as the observed means <span class="math inline">\(r_i\)</span> and variances <span class="math inline">\(σ_{r_i}^2\)</span>.</p>
<p>The mean and variance of the Gaussian prior are learned as free-standing
variables using the backpropagation during training. Both the prior variance as
well as the variance of the observations are rectified to enforce positivity
using <span class="math inline">\(\text{softplus}\)</span>.</p>
<p>To get the observed elements <span class="math inline">\(r_i\)</span>, we use the two encoder networks <span class="math inline">\(enc_r\)</span> and
<span class="math inline">\(enc_σ\)</span> that consist of a set of dense layers:</p>
<p><span class="math display">\[r_i = \text{enc}_r(o_{k→g_i}), \quad σ_{r_i} = \text{enc}_σ(o_{k→g_i})\]</span></p>
<p><span class="math inline">\(\text{enc}_r\)</span> and <span class="math inline">\(\text{enc}_σ\)</span> are either separate dense neural networks or a single dense
neural network with the output having two scalars per feature (one for the mean,
one for the variance). Finally we retrieve the value of <span class="math inline">\(e_{k→G}\)</span> from the
aggregated latent variable, using either just the mean of <span class="math inline">\(z\)</span>:</p>
<p><span class="math display">\[e_{k→G} = μ_z\]</span></p>
<p>or by concatenating the mean and the variance:</p>
<p><span class="math display">\[e_{k→G} = (μ_z, σ_z^2).\]</span></p>
<p>The mean and variance are calculated from the conditioned Gaussian based on the
encoded observations as described in <a href="#sec:bayesianagg1">Section 1.4.5.4</a>:</p>
<p><span class="math display">\[\sigma_z^2 = \frac{1}{\frac{1}{\sigma_{z_0}^2} + \sum_{i=1}^n{\frac{1}{\text{enc}_σ(o_{k→g_i})^2}}}\]</span></p>
<p><span class="math display">\[\mu_z = \mu_{z_0} + \sigma_z^2 \cdot \sum_{i=1}^{n}{\frac{(\text{enc}_r(o_{k→g_i})-\mu_{z_0})}{\text{enc}_σ(o_{k→g_i})^2}}\]</span></p>
<p>A graphical overview of this method is shown in <a href="#fig:bayesianagg">Figure 2</a>.</p>
<figure>
<img src="images/bayesianagg.drawio.svg" id="fig:bayesianagg" alt="Figure 2: Bayesian Aggregation in graphical form. The observations from the neighboring agents are encoded with the value encoder and the confidence encoder. They are then used to condition the Gaussian prior estimate of the latent variable z to get the final mean and variance of the a-posteriori estimate. The mean and optionally the variance estimate of z are concatenated with the latent spaces from the other aggregatables and passed to the decoder as shown in Figure 1." /><figcaption aria-hidden="true">Figure 2: Bayesian Aggregation in graphical form. The observations from the neighboring agents are encoded with the value encoder and the confidence encoder. They are then used to condition the Gaussian prior estimate of the latent variable <span class="math inline">\(z\)</span> to get the final mean and variance of the a-posteriori estimate. The mean and optionally the variance estimate of <span class="math inline">\(z\)</span> are concatenated with the latent spaces from the other aggregatables and passed to the decoder as shown in <a href="#fig:model">Figure 1</a>.</figcaption>
</figure>
<h4 id="attentive-aggregation">Attentive Aggregation</h4>
<p>When using residual self-attentive aggregation, we define the aggregated feature
as</p>
<p><span class="math display">\[e_{k→g} = \frac{1}{n} \sum_{i=1}^n \text{resatt}(\text{enc}(o_i))\]</span></p>
<p>with <span class="math inline">\(\text{enc}\)</span> being a dense neural network with zero or more hidden layers,
and</p>
<p><span class="math display">\[\text{resatt}(o_i) = o_i + \text{dense}(\text{MHA}(o_i, o_i, o_i)).\]</span></p>
<p><em>Residual</em> means that the input is added to the output of the
attention mechanism, so the attention mechanism only has to learn a <em>residual</em>
value that modifies the input features.</p>
<p>The <span class="math inline">\(\text{MHA}\)</span> module is the multi-head attention module from
<span class="citation" data-cites="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">[<a href="#ref-https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" role="doc-biblioref">VSPU21</a>]</span> as described in <a href="#sec:mha">Section 1.4.5.5</a>.</p>
<p>This method of attentive aggregation is similar to the method successfully used
by <span class="citation" data-cites="https://arxiv.org/abs/1909.07528">[<a href="#ref-https://arxiv.org/abs/1909.07528" role="doc-biblioref">BKMW21</a>]</span>. An overview over the model architecture used in <span class="citation" data-cites="https://arxiv.org/abs/1909.07528">[<a href="#ref-https://arxiv.org/abs/1909.07528" role="doc-biblioref">BKMW21</a>]</span> can be
seen in <a href="#fig:openai">Figure 3</a>.</p>
<figure>
<img src="images/model-openai.drawio.svg" id="fig:openai" alt="Figure 3: A schematic of the model architecture used by OpenAI [BKMW21] using masked residual self-attention. It is similar to our architecture (Figure 1) except for the LIDAR in the self-observations as well as the LSTM layer at the end." /><figcaption aria-hidden="true">Figure 3: A schematic of the model architecture used by OpenAI <span class="citation" data-cites="https://arxiv.org/abs/1909.07528">[<a href="#ref-https://arxiv.org/abs/1909.07528" role="doc-biblioref">BKMW21</a>]</span> using masked residual self-attention. It is similar to our architecture (<a href="#fig:model">Figure 1</a>) except for the LIDAR in the self-observations as well as the LSTM layer at the end.</figcaption>
</figure>
<h2 id="sec:experiments">Experiments</h2>
<h3 id="experimental-setup">Experimental Setup</h3>
<p>All of our experiments were run using stable-baselines3 (sb3). <em>OpenAI
Baselines</em> is a deep reinforcement learning framework that provides
implementations of various reinforcement learning algorithms for TensorFlow.
Stable-baselines is an extension of the original code base with better
documentation and more flexibility for custom architectures and environments.
sb3 is the continuation of the stable-baselines project, rewritten using
PyTorch.</p>
<p>We extended sb3 for the multi-agent use case by adapting the vectorized
environment implementation to support multiple agents in a single environment,
as well as adapting the training and evaluation functionality to correctly
handle the centralized-learning decentralized-execution method. We also
implement Trust Region Layers <span class="citation" data-cites="https://openreview.net/forum?id_x61_qYZD-AO1Vn">[<a href="#ref-https://openreview.net/forum?id_x61_qYZD-AO1Vn" role="doc-biblioref">OBNZ21</a>]</span> as a new training algorithm for sb3 in
order to be able to directly compare PPO and TRL.</p>
<p>We optimized the hyper parameters using Optuna
<span class="citation" data-cites="https://dl.acm.org/doi/10.1145/3292500.3330701">[<a href="#ref-https://dl.acm.org/doi/10.1145/3292500.3330701" role="doc-biblioref">ASYO21</a>]</span>.</p>
<p>{describe batch size, other meta settings}</p>
<h3 id="considered-tasks">Considered Tasks</h3>
<p>To evaluate the different aggregation methods we need simulated environments
where multiple agents can cooperatively solve a task. Since most of the commonly
used tasks used to benchmark reinforcement learning algorithms are designed for
a single agent, we use custom built environments.</p>
<p>The following shows which specific tasks we consider.</p>
<h4 id="sec:rendezvous">Rendezvous Task</h4>
<p>In the rendezvous task, a set of <span class="math inline">\(n\)</span> agents try to meet up in a single point. An
example is shown in <a href="#fig:rendezvous1">Figure 4</a>. Our implementation of the rendezvous task
is modeled after <span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span>.</p>
<figure>
<img src="images/rendezvous1.png" id="fig:rendezvous1" alt="Figure 4: Visualization of one successful episode of the rendezvous task (from [HüŠoNe21])" /><figcaption aria-hidden="true">Figure 4: Visualization of one successful episode of the rendezvous task (from <span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span>)</figcaption>
</figure>
<p>The agents are modeled as infinitesimal dots without collisions. They use
double-integrator unicycle dynamics
<span class="citation" data-cites="https://ieeexplore.ieee.org/document/976029">[<a href="#ref-https://ieeexplore.ieee.org/document/976029" role="doc-biblioref">EgHu21</a>]</span>, so the action outputs are the
acceleration of the linear velocity (<span class="math inline">\(\dot{v}\)</span>) and the angular velocity
(<span class="math inline">\(\dot{ω}\)</span>). The agents move around in a square world that either has walls at
the edge or is on a torus (the position is calculated modulo <span class="math inline">\(100\)</span>).</p>
<p>The observation space of agent <span class="math inline">\(a_i\)</span> comprises the following information:</p>
<ol type="1">
<li>The own linear velocity <span class="math inline">\(v_{a_i}\)</span></li>
<li>The own angular velocity <span class="math inline">\(ω_{a_i}\)</span></li>
<li>The ratio of currently visible agents</li>
<li>If the observation space is not a torus: the distance and angle to the
nearest wall</li>
<li>For each neighboring agent:
<ol type="1">
<li>The distance between the current agent and the neighbor</li>
<li>The cos and sin of the relative bearing (the angle between the direction
the agent is going and the position of the neighbor)</li>
<li>The cos and sin of the neighbor’s bearing to us</li>
<li>The velocity of the neighbor relative to the agent’s velocity</li>
<li>The ratio of currently visible agents for the neighbor</li>
</ol></li>
</ol>
<p>The reward that every agent gets is the mean of all the pairwise agent
distances:</p>
<p><span class="math display">\[r = \frac{1}{n} \sum_{i=0}^n \sum_{j=i+1}^n ||p_i-p_j||\]</span></p>
<p>The episode ends after 1024 time steps.</p>
<h4 id="single-evader-pursuit-task">Single-Evader Pursuit Task</h4>
<p>In the pursuit task, multiple pursuers try to catch an evader. The evader agent
has a higher speed than the pursuers, so the pursuers need to cooperate to be
able to catch it. The world is a two-dimensional torus (the position <span class="math inline">\((x, y)\)</span>
are floating point numbers modulo 100). If the world was infinite, the evader
could run away in one direction forever, and if it had walls, the pursuers could
easily corner the evader. The pursuit tasks are modeled after <span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span>. An
example episode of the pursuit task is in <a href="#fig:pursuit1">Figure 5</a>.</p>
<figure>
<img src="images/pursuit1.png" id="fig:pursuit1" alt="Figure 5: Visualization of one successful episode of the pursuit task (from [HüŠoNe21]). The pursuers are in blue, the evader is in red." /><figcaption aria-hidden="true">Figure 5: Visualization of one successful episode of the pursuit task (from <span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span>). The pursuers are in blue, the evader is in red.</figcaption>
</figure>
<p>The agents are modeled with single-integrator unicycle dynamics. The action
outputs are the linear velocity (<span class="math inline">\(v\)</span>) and the angular velocity (<span class="math inline">\(ω\)</span>).</p>
<p>The evader is not part of the training and uses a hard-coded algorithm based on
maximizing Voronoi-regions as described by
<span class="citation" data-cites="https://www.sciencedirect.com/science/article/abs/pii/S0005109816301911">[<a href="#ref-https://www.sciencedirect.com/science/article/abs/pii/S0005109816301911" role="doc-biblioref">ZZDH21</a>]</span>. It
is thus part of the environment and not an “agent” as seen from the perspective
of the training algorithm.</p>
<p>The observation space for the single-evader pursuit task comprises the following
information:</p>
<ol type="1">
<li>The ratio of agents that are visible</li>
<li>For each neighboring agent:
<ol type="1">
<li>The distance between the current agent and the neighbor</li>
<li>The cos and sin of the relative bearing (the angle between the direction
the agent is going and the position of the neighbor)</li>
<li>The cos and sin of the neighbor’s bearing to us</li>
</ol></li>
<li>For each evader:
<ol type="1">
<li>The distance between the current agent and the evader</li>
<li>The cos and sin of the relative bearing</li>
</ol></li>
</ol>
<p>The reward function of all the agents is the minimum distance of any agent to
the evader:</p>
<p><span class="math display">\[r = min_{i=0}^n ||p_{a_i} - p_{e}||\]</span></p>
<p>The episode ends once the evader is caught or 1024 timesteps have passed. The
evader is declared as caught if the distance is minimum distance between an
agent and the evader is less than <span class="math inline">\(1%\)</span> of the world width.</p>
<h5 id="multi-evader-pursuit">Multi-Evader Pursuit</h5>
<p>The multi-evader pursuit task is the same as the normal pursuit task, except
there are multiple hard-coded evaders.</p>
<p>The reward here is different, since defining the reward as for the single-evader
task is not obvious. The reward is +1 whenever an evader is caught, 0 otherwise.
An evader is declared caught if the distance to the nearest pursuer is less than
<span class="math inline">\(\frac{2}{100}\)</span> of the world width. Contrary to the single-evader task, the
episode does not end when an evader is caught and instead always runs for 1024
timesteps.</p>
<h5 id="box-assembly-task">Box Assembly Task</h5>
<p>In the box assembly task, the agents are modeled similar to Kilobots, as
described by <span class="citation" data-cites="https://www.semanticscholar.org/paper/Using-M-Embeddings-to-Learn-Control-Strategies-for-Gebhardt-H_x37_C3_x37_BCttenrauch/9f550815f8858e7c4c8aef23665fa5817884f1b3">[<a href="#ref-https://www.semanticscholar.org/paper/Using-M-Embeddings-to-Learn-Control-Strategies-for-Gebhardt-H_x37_C3_x37_BCttenrauch/9f550815f8858e7c4c8aef23665fa5817884f1b3" role="doc-biblioref">GeHüNe21</a>]</span>. The agents are two-dimensional circles with collision.</p>
<p>For simplicity, we again use single-integrator unicycle dynamics with the linear
velocity <span class="math inline">\(v\)</span> and the angular velocity <span class="math inline">\(ω\)</span> as the action outputs instead of the
specific dynamics of real Kilobots.</p>
<p>The world is a square and contains a few two-dimensional boxes (squares) as
obstacles. For the box assembly task, the goal is to get all the boxes as close
together as possible. Since they agents are much smaller than the boxes, moving
a box is hard for a single agent. The reward is the negative sum of the pairwise
distances between the boxes. We run this task with four boxes and 10 agents.</p>
<p>An example of a successful episode of the task is in <a href="#fig:assembly">Figure 6</a>.</p>
<figure>
<img src="images/box-assembly.png" id="fig:assembly" alt="Figure 6: Example successful episode of the box assembly task (from [GeHüNe21])" /><figcaption aria-hidden="true">Figure 6: Example successful episode of the box assembly task (from <span class="citation" data-cites="https://www.semanticscholar.org/paper/Using-M-Embeddings-to-Learn-Control-Strategies-for-Gebhardt-H_x37_C3_x37_BCttenrauch/9f550815f8858e7c4c8aef23665fa5817884f1b3">[<a href="#ref-https://www.semanticscholar.org/paper/Using-M-Embeddings-to-Learn-Control-Strategies-for-Gebhardt-H_x37_C3_x37_BCttenrauch/9f550815f8858e7c4c8aef23665fa5817884f1b3" role="doc-biblioref">GeHüNe21</a>]</span>)</figcaption>
</figure>
<p>The observation space for the box assembly task contains the following
information:</p>
<ol type="1">
<li>The absolute position (x,y) of the current agent</li>
<li>The sin, cos of the absolute rotation of the current agent</li>
<li>For every neighboring agent:
<ol type="1">
<li>The distance between the current agent and the neighbor</li>
<li>The sin and cos of the bearing angle to the neighbor</li>
<li>The sin and cos of the neighbor’s rotation relative to our rotation</li>
</ol></li>
<li>For every neighboring box:
<ol type="1">
<li>The distance between the current agent and the box</li>
<li>The sin and cos of the bearing angle to the box</li>
<li>The sin and cos of the box’s rotation relative to our rotation</li>
</ol></li>
</ol>
<h5 id="box-clustering-task">Box Clustering Task</h5>
<p>The task setup for box clustering is the same as for the box assembly task,
except that each box is assigned a color. The goal is to move the boxes into an
arrangement such that boxes of the same color are as close together as possible,
while boxes of different colors are far away. The reward is the negative sum of
pairwise distances between the boxes of each cluster plus the sum of pairwise
distances of the center of mass of each cluster.</p>
<p>The observation space is the same as the observation space of the box assembly
task, except that each cluster of objects is aggregated into a separate
aggregation space.</p>
<p>{example clustering episode}</p>
<!-- ### Other considered tasks

PettingZoo tasks (without results?)

-->
<h2 id="sec:results">Results</h2>
<p>In this section, we present the results of our experiments. We compare (1) the
training performance of the different aggregation methods on various tasks, (2)
aggregating different observation sets into a single vs multiple latent spaces
(3) various variants of Bayesian aggregation (4) the training algorithms PPO and
TRL.</p>
<p>In order to compare the performance of different variants, we consider the
average reward per episode over the agent steps used during training. This way
we can evaluate and compare the overall best results as well as the sample
efficiency.</p>
<p>Each policy gradient training step consists of multiple gradient descent steps
with a set of batches of trajectory rollouts generated using the previous
policy. We evaluate the performance after each training step on a separate set
of evaluation environments.</p>
<p>Unless mentioned otherwise, the following setup is used:</p>
<ul>
<li>Plotted is the median reward of the n runs at a specific training step, the
error band is the 25th and 75th percentile of runs</li>
<li>The activation function used after each layer is LeakyReLU</li>
<li>For the Bayesian aggregation:
<ul>
<li>We only use the mean value <span class="math inline">\(μ_z\)</span> as an output, not <span class="math inline">\(μ_z\)</span> and <span class="math inline">\(σ_z^2\)</span></li>
<li>We use a single shared encoder for the value and confidence estimates</li>
<li>The a-priori estimate <span class="math inline">\(μ_{z_0}\)</span> is learnable separately for each feature
dimension</li>
<li>The variance of both the a-priori estimate as well as the encoded estimates
are rectified using <span class="math inline">\(\text{softplus}\)</span>.</li>
</ul></li>
<li>Multiple aggregatable groups are aggregated into separate latent spaces</li>
<li>The parameters of the policy and value function are not shared</li>
<li>The training algorithm used is PPO</li>
</ul>
<p>Due to time and resource constraints, we only use individually hyper-parameter
optimized architectures on the multi-evader pursuit and rendezvous tasks, and
use the same architecture for all aggregation methods on the other tasks.</p>
<h3 id="aggregation-method-results">Aggregation Method Results</h3>
<p>We compare the performance of the different aggregation methods (Bayesian
aggregation, mean aggregation, attentive aggregation) on multiple tasks.</p>
<p>We use the notation <code>64-agg-64</code> to describe the layer sizes of the neural
network: The numbers before <code>agg</code> are the sequential layer sizes of the dense
layers of the encoders of each aggregation group. The numbers after <code>agg</code> are
the layer sizes in the decoder after the concatentation of the proprioceptive
observations with the aggregated observations (compare <a href="#fig:model">Figure 1</a>).</p>
<h4 id="multi-evader-pursuit-task">Multi-Evader Pursuit Task</h4>
<p>Here, we consider the multi-evader pursuit task with 20 pursuers and 5 evaders
on a torus. <a href="#fig:resmpsmall">Figure 7</a> shows the results of the multi-evader pursuit task
with different aggregation methdos with the same architecture used in
<span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span> to be able to directly compare the results. The architecture is
64-agg-64 with the tanh activation function. With this architecture, the
Bayesian aggregation performs best.</p>
<figure>
<img src="images/plots/2021-07-10_13.30.06-Multi-Evader%20Pursuit%20(smallarch).svg" id="fig:resmpsmall" alt="Figure 7: Results on the multi-evader pursuit task with the NN architecture adapted from [HüŠoNe21]. The Bayesian aggregation performs best." /><figcaption aria-hidden="true">Figure 7: Results on the multi-evader pursuit task with the NN architecture adapted from
<span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span>. The Bayesian aggregation performs
best.</figcaption>
</figure>
<p><a href="#fig:resmpopt">Figure 8</a> shows the results with neural network architectures that were
separately hyper-parameter optimized for each aggregation method except max
aggregation. The optimized architecture for the mean aggregation is
<code>174-226-97-agg-96</code>, the same architecture is used for the max aggregation. The
optimized architecture for the Bayesian aggregation is <code>120-60-agg-160</code>. The
optimized architecture for the attentive aggregation is <code>72-agg-132-200</code>. All
optimized architectures use the LeakyReLU activation functions. Note that the
architecture optimized on the mean aggregation is the deepest with three hidden
layers before the aggregation, while the optimized architecture on the attentive
aggregation has multiple layers after the aggregation instead. With the
hyper-parameter optimized architecture, the mean aggregation performs best. The
results are still similar when using the same <code>120-60-agg-160</code> architecture for
every aggregation method. These results of Figures <a href="#fig:resmpsmall">7</a>, <a href="#fig:resmpopt">8</a>
indicate that the Bayesian aggregation outperforms the mean aggregation when the
neural network is limited in size, but has no advantage when the neural network
is sufficiently large and deep. The neural network seems to be able to
implicitly learn to transform and weigh the information from the different
observables, compensating the advantage of the additional structure of relevancy
/ certainty that is given in the Bayesian aggregation.</p>
<p><a href="#fig:resmpopttop">Figure 9</a> shows the results of the top third of runs. The performane of
the mean and Bayesian aggregation is similar, indicating that the best runs are
similar, but that the Bayesian aggregation has more runs that fail to achieve
the peak performance.</p>
<figure>
<img src="images/plots/2021-07-22_15.16.03-Multi-Evader%20Pursuit%20(hpsopt).svg" id="fig:resmpopt" alt="Figure 8: Results on the multi-evader pursuit task with the NN architecture hyper-parameter optimized for each aggregation method. The mean aggregation performs best." /><figcaption aria-hidden="true">Figure 8: Results on the multi-evader pursuit task with the NN architecture hyper-parameter optimized for each aggregation method. The mean aggregation performs best.</figcaption>
</figure>
<figure>
<img src="images/plots/2021-07-10_16.07.12-Multi-Evader%20Pursuit%20(hpsopt%20top.33).svg" id="fig:resmpopttop" alt="Figure 9: Like Figure 8 but only the top 1/3 of runs. This shows that the peak performance of the mean and the Bayesian aggregation is similar." /><figcaption aria-hidden="true">Figure 9: Like <a href="#fig:resmpopt">Figure 8</a> but only the top 1/3 of runs. This shows that the peak performance of the mean and the Bayesian aggregation is similar.</figcaption>
</figure>
<h4 id="single-evader-pursuit-task-1">Single-Evader Pursuit Task</h4>
<p><a href="#fig:ressp">Figure 10</a> shows the results on the single-evader pursuit task with 10 pursuers
and one evader. The neural network architecture is fixed at <code>120-60-agg-160</code> for
all methods. All methods learn the task quickly, with the mean aggregation
achieving the maximum performance slightly faster. This shows that the task is
simpler than the multi-evader pursuit task, which is both due to the fact that
there are fewer evaders and that the reward is more sparse (minimum-distance for
single-evader vs count-catches for multi-evader).</p>
<figure>
<img src="images/plots/2021-07-14_13.55.20-Single-evader%20Pursuit.svg" id="fig:ressp" alt="Figure 10: Results on Single Pursuit task. Architecture: 120-60-agg-160." /><figcaption aria-hidden="true">Figure 10: Results on Single Pursuit task. Architecture: 120-60-agg-160.</figcaption>
</figure>
<h4 id="rendezvous-task">Rendezvous Task</h4>
<p><a href="#fig:resrendezvous">Figure 11</a> shows a comparison between mean and Bayesian aggregation on
the rendezvous task with twenty agents in a two dimensional square world with
walls. The medium architecture is the one optimized on the pursuit task
(<code>120-60-agg-160</code>). The small architecture is the one used in <span class="citation" data-cites="https://jmlr.org/beta/papers/v20/18-476.html">[<a href="#ref-https://jmlr.org/beta/papers/v20/18-476.html" role="doc-biblioref">HüŠoNe21</a>]</span>
(<code>64-agg-64</code>). The optimized architecture was optimized on the rendezvous task
directly: <code>146-120-agg-19-177-162</code>. All architectures and aggregation methods
successfully solve the task. The aggregation method does not make any difference
in the performance, but both the small and the medium architecture have a “drop”
in training speed at around 2 million steps, while the optimized architecture
smoothly learns the problem. The logarithmic scale graph to the right shows that
while the small and optimized architecture both reach the same final score, the
medium architecture never reaches the same score. This might be because both the
small and the optimized architecture have a bottleneck layer after the
aggregation, forcing the neural network to simplify the strategy.</p>
<figure>
<img src="images/plots/2021-07-22_15.51.34-Rendezvous.svg" id="fig:resrendezvous" alt="Figure 11: Results on the rendezvous task. The results barely differ between the mean and Bayesian aggregation, but the size of the policy architecture makes a difference. In the logarithmic view on the right it can be seen that the medium architecture does not reach the same final performance as the other two architectures." /><figcaption aria-hidden="true">Figure 11: Results on the rendezvous task. The results barely differ between the mean and Bayesian aggregation, but the size of the policy architecture makes a difference. In the logarithmic view on the right it can be seen that the medium architecture does not reach the same final performance as the other two architectures.</figcaption>
</figure>
<!--
![Detail of the results on the rendezvous task in a logarithmic scale. The medium architecture does not reach the same final performance as the other two architectures.](images/plots/2021-07-22_15.54.43-Rendezvous
Log.svg){#fig:resrendezvouslog}

-->
<h4 id="assembly-task">Assembly Task</h4>
<p><a href="#fig:resassembly">Figure 12</a> shows the results on the assembly task with ten agents and four
boxes. The three aggregation methods perform very similar, with the attentive
aggregation learning the task slightly quicker.</p>
<figure>
<img src="images/plots/2021-07-22_17.44.09-assembly%20(by%20agg%20method).svg" id="fig:resassembly" alt="Figure 12: Results on the assembly task." /><figcaption aria-hidden="true">Figure 12: Results on the assembly task.</figcaption>
</figure>
<h4 id="clustering-task-with-two-clusters">Clustering Task With Two Clusters</h4>
<p><a href="#fig:resclustering2">Figure 13</a> shows the results on the clustering task with four boxes
split into two clusters.</p>
<figure>
<img src="images/plots/2021-07-10_18.56.32-Clustering%20task%20(2%20clusters,%20by%20agg%20method).svg" id="fig:resclustering2" alt="Figure 13: Clustering2 results" /><figcaption aria-hidden="true">Figure 13: Clustering2 results</figcaption>
</figure>
<h4 id="clustering-task-with-three-clusters">Clustering Task With Three Clusters</h4>
<p>Doesn’t work :(</p>
<h3 id="learning-algorithm-comparison-ppo-vs-pg-trl">Learning Algorithm Comparison (PPO vs PG-TRL)</h3>
<p>In the following, we show some results of the trust region layers policy
gradient (TRL) training method (see <a href="#sec:trl">Section 1.4.2.2</a>) compared to PPO.</p>
<p><a href="#fig:resmptrl">Figure 14</a> shows the learning algorithm comparison on the multi-evader
pursuit task. The architecture are the ones hyper-parameter optimized on PPO on
each of the aggregation methods. TRL seems to show significantly improved
training performance for the Bayesian aggregation and similar performance for
the mean aggregation.</p>
<!-- @Fig:resmptrltop
shows the same result for only the top one third of runs. The results are very
similar. This indicates that TRL makes the training more stable on this task,
fewer runs fail to achieve the optimal performance. -->
<figure>
<img src="images/plots/2021-07-22_16.26.13-Multi-Evader%20Pursuit%20TRL.svg" id="fig:resmptrl" alt="Figure 14: TRL vs PPO learning algorithms on the multi-evader pursuit task. For Bayesian aggregation, the training is faster and the end result better. For mean aggregation, the results are fairly similar." /><figcaption aria-hidden="true">Figure 14: TRL vs PPO learning algorithms on the multi-evader pursuit task. For Bayesian aggregation, the training is faster and the end result better. For mean aggregation, the results are fairly similar.</figcaption>
</figure>
<!--
![TRL vs PPO (multi-evader pursuit). Top 1/3 of runs](images/plots/2021-07-11_13.23.22-Multi-Evader
Pursuit TRL Top.33.svg){#fig:resmptrltop}
-->
<p><a href="#fig:ressptrl">Figure 15</a> shows the comparison on the single-evader pursuit task. Here, the
performance of TRL is better than PPO on both the mean and the Bayesian
aggregation.</p>
<figure>
<img src="images/plots/2021-07-15_12.57.59-Single-evader%20Pursuit%20TRL%20vs%20PPO.svg" id="fig:ressptrl" alt="Figure 15: TRL vs PPO learning algorithms on single-evader pursuit task. The performance of TRL is better for both the mean and the Bayesian aggregation." /><figcaption aria-hidden="true">Figure 15: TRL vs PPO learning algorithms on single-evader pursuit task. The performance of TRL is better for both the mean and the Bayesian aggregation.</figcaption>
</figure>
<p><a href="#fig:resasstrl">Figure 16</a> shows the comparison on the assembly task. All aggregation
methods perform the same, except the Bayesian aggregation with TRL, which is
worse.</p>
<figure>
<img src="images/plots/2021-07-11_13.13.37-assembly%20(by%20train%20algo%20and%20agg%20method).svg" id="fig:resasstrl" alt="Figure 16: TRL vs PPO learning algorithms on the assembly task. The training performance is the same for all aggregation methods, except that the Bayesian aggregation performs worse with TRL." /><figcaption aria-hidden="true">Figure 16: TRL vs PPO learning algorithms on the assembly task. The training performance is the same for all aggregation methods, except that the Bayesian aggregation performs worse with TRL.</figcaption>
</figure>
<p>In summary, TRL seems to perform the same or better in most cases, with the
Bayesian aggregation on the assembly task being an outlier.</p>
<h3 id="same-space-vs-separate-space-aggregation">Same Space vs Separate Space Aggregation</h3>
<p>For tasks where we have multiple aggregation groups, we can also aggregate all
observables into the same latent space instead of separate ones. This means that
instead of each aggregation space containing the information of those
aggregatables, the single aggregation space must contain the information of all
observables as it pertains to the current agent. The potential advantage is that
the policy can share more parameters and thus be more sample efficient. It can
also scale to a larger number of categories of objects (aggregation groups).
<a href="#fig:sameseparate">Figure 17</a> shows a schematic comparison between the two methods.</p>
<p>In the other experiments we alwas use separate spaces. <a href="#fig:ressameseparate">Figure 18</a>
shows the results of aggregating into a single space vs. into separate spaces
for the multi-evader pursuit and assembly tasks. In the case of the multi-evader
pursuit task same-space aggregation means that both the neighboring pursuers as
well as all the evaders are aggregated into one space. For the assembly task,
both the neighboring agents as well as the boxes are aggregated into one space.
The separate-space aggregation performs better in both tasks, with a higher
margin in the multi-evader task.</p>
<p>In summary, while the same-space aggregation is promising by in theory being
able to scale to more complex environment, it performs worse for our tasks. This
might be due to the fact that learning different encoder networks to output
information into the same latent space is hard. An alternative explanation might
be that our experiments only having few aggregation groups so the value of
parameter sharing is low, or due to the lack of experiments with more different
hyper-parameters.</p>
<figure>
<img src="images/model-sameseparate.drawio.svg" id="fig:sameseparate" alt="Figure 17: Schematic comparison of separate space vs. same-space aggregation. In same-space aggregation the encoders are still separate, but the latent space is shared. The same-space aggregation shares more parameters and can scale better to more aggregation groups." /><figcaption aria-hidden="true">Figure 17: Schematic comparison of separate space vs. same-space aggregation. In same-space aggregation the encoders are still separate, but the latent space is shared. The same-space aggregation shares more parameters and can scale better to more aggregation groups.</figcaption>
</figure>
<figure>
<img src="images/plots/2021-07-22_16.05.47-samespace.svg" id="fig:ressameseparate" alt="Figure 18: Separate vs same-space aggregation on the multi-evader pursuit and assembly tasks. The same-space aggregation performs worse in both tasks." /><figcaption aria-hidden="true">Figure 18: Separate vs same-space aggregation on the multi-evader pursuit and assembly tasks. The same-space aggregation performs worse in both tasks.</figcaption>
</figure>
<!--
![Multi-pursuit same space comparison](images/plots/2021-07-14_13.37.01-Multi-Evader
Pursuit samespace.svg){#fig:ressameseparate}

![Assembly task same space comparison](images/plots/2021-07-14_13.19.13-Assembly
samespace.svg){#fig:ressameseparate2}

-->
<h3 id="bayesian-aggregation-variants">Bayesian Aggregation Variants</h3>
<p>The following shows results for some variants of the Bayesian aggregation.</p>
<h4 id="separate-vs-common-encoder">Separate vs Common Encoder</h4>
<p>As described in <a href="#sec:bayesianagg">Section 1.6.1.2</a>, we can either have a shared encoder to predict
the mean and variance of each sample in each aggregation space by making the
last layer of the encoder have two outputs for each feature, or have two fully
separate networks (<span class="math inline">\(enc_r\)</span> and <span class="math inline">\(enc_σ\)</span>). In our experiments, using one common
encoder with two outputs generally performs better.</p>
<h4 id="using-the-aggregated-variance-or-only-the-mean">Using the Aggregated Variance or Only the Mean</h4>
<p>In the other experiments with Bayesian aggregation, we only use the predicted
mean of the Gaussian distribution as an input to the decoder:</p>
<p><span class="math display">\[e_{k→G}=μ_z\]</span></p>
<p>Since the Bayesian aggregation also gives us a full a-posteriori Gaussian
distribution, we also have an estimate of the variance for each feature in the
latent space that is computed from the apriori variance conditioned on each seen
latent space sample. We can feed this variance to the decoder by concatenating
it with the mean predictions in the hope that the neural network is able to use
this additional information:</p>
<p><span class="math display">\[e_{k→G}=(μ_z, σ_z^2)\]</span></p>
<p>The results of applying this method to the multi-evader pursuit are seen in
<a href="#fig:resoutputvariance">Figure 19</a>. The neural network architecture is the same as for the
other experiments with Bayesian aggregation on multi-evader pursuit
(<code>120-60-agg-160</code>). Including the variance in the decoder inputs decreases the
performance.</p>
<p>The decreasing performance could be a result of the increased dimension of the
decoder inputs. Adding the variance inputs doubles the number of values the
decoder has to process and learn from. Since the structure of the encoded values
and variances can not known beforehand to the decoder, it has to learn to
interpret more information than when receiving just the mean values. The added
variance inputs should give the decoder the ability to understand the confidence
of each of the value predictions and weigh them accordingly, but the added
complexity seems to make it not worth it.</p>
<figure>
<img src="images/plots/2021-07-11_12.27.17-Pursuit%20(bayes%20outputvariance).svg" id="fig:resoutputvariance" alt="Figure 19: Results of Bayesian aggregation on the multi-evader pursuit task, depending on whether the variance is also fed into the decoder or only the mean." /><figcaption aria-hidden="true">Figure 19: Results of Bayesian aggregation on the multi-evader pursuit task, depending on whether the variance is also fed into the decoder or only the mean.</figcaption>
</figure>
<!-- ### Local obs aggregation space -->
<!-- ### Activation functions

(probably uninteresting)

-->
<!--# Implementation Details-->
<h2 id="sec:conclusion">Conclusion and Future Work</h2>
<h3 id="conclusion">Conclusion</h3>
<p>We have made a comprehensive comparison between the performance of mean
aggregation, Bayesian aggregation, and attentive aggregation to collect a
varying number of observations on a set of different deep reinforcement learning
tasks. We have observed that there is no clear advantage of one of the methods
over the others, with the results differing strongly between the different
tasks.</p>
<!-- In general, the signal to noise ratio of the experiments was pretty low, -->
<p>We have also shown the results of a few variants of the Bayesian aggregation and
concluded that encoding the variance with the same encoder as the estimate,
aggregating into separate latent spaces and not using the aggregated variance as
an input to the decoder achieves the best results.</p>
<p>Finally, we have applied a new training method (trust region layers) to
multi-agent reinforcement learning and compared it to the commonly used PPO. The
results indicate that TRL is usually at least as good as PPO and in some cases
outperforms it.</p>
<h3 id="future-work">Future Work</h3>
<p>There are many avenues for future work in this area.</p>
<p>All of our experiments used global visibility due to the noisy nature of limited
local visibility making it harder to make any strong conclusions. Since the
local visibility case also increases the uncertainty of each observation though,
it might be a case where Bayesian aggregation performs better. Future work
should include experiments that have a larger environment with observability
limited by range or by obstacles.</p>
<p>Even though we show in our experiments that Bayesian aggregation into the same
latent space is worse than aggregating into separate spaces, there might be
potential there to be able to scale the number of aggregation groups with more
hyperparameter tuning or by introducing a two-stage encoder where the first
encoder is separate by aggregation group and the second encoder is shared, then
aggregating the output of the second encoder into the same space.</p>
<p>We also only considered tasks with implicit communication - the agents had to
infer the intent of the other agents purely by their actions. There is related
work that adds explicit communication between agents that is learned together
with the policy. This is usually implemented as another action output that is
written directly into the observation of the other agents instead of affecting
the world. Explicit communication architectures may be able to handle some tasks
better than those with implicit communication, but they are often only
applicable to environments with exactly two agents. For more agents, the
performance of explicit communication architectures may be affected by the
aggregation methods used and thus might be a use case for Bayesian aggregation.</p>
<ul>
<li>recurrent architecture</li>
</ul>
<h2 class="page_break_before unnumbered" id="bibliography">Bibliography</h2>
<!-- Explicitly insert bibliography here -->
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-https://link.springer.com/article/10.1007/s43154-021-00048-3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[Drew21] </div><div class="csl-right-inline"><strong>Multi-Agent Systems for Search and Rescue Applications</strong> <div class="csl-block">Daniel S Drew</div> <em>Current Robotics Reports</em> (2021-06) <a href="https://doi.org/10.1007/s43154-021-00048-3">https://doi.org/10.1007/s43154-021-00048-3</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s43154-021-00048-3">10.1007/s43154-021-00048-3</a></div></div>
</div>
<div id="ref-https://ieeexplore.ieee.org/abstract/document/9173524" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[HTKL21] </div><div class="csl-right-inline"><strong>Occlusion-Based Coordination Protocol Design for Autonomous Robotic Shepherding Tasks</strong> <div class="csl-block">Junyan Hu, Ali Emre Turgut, Tomáš Krajník, Barry Lennox, Farshad Arvin</div> <em>IEEE Transactions on Cognitive and Developmental Systems</em> (2020) <a href="https://ieeexplore.ieee.org/abstract/document/9173524">https://ieeexplore.ieee.org/abstract/document/9173524</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/TCDS.2020.3018549">10.1109/tcds.2020.3018549</a></div></div>
</div>
<div id="ref-https://doi.org/10.3929/ethz-a-010831954" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[WaKeAu21] </div><div class="csl-right-inline"><strong>Drone shows: Creative potential and best practices</strong> <div class="csl-block">Markus Waibel, Bill Keays, Federico Augugliaro</div> <em>ETH Zurich</em> (2017) <a href="https://www.research-collection.ethz.ch/handle/20.500.11850/125498">https://www.research-collection.ethz.ch/handle/20.500.11850/125498</a> <div class="csl-block">DOI: <a href="https://doi.org/10.3929/ethz-a-010831954">10.3929/ethz-a-010831954</a></div></div>
</div>
<div id="ref-https://doi.org/10.1117/12.830408" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[Bürk21] </div><div class="csl-right-inline"><strong>Collaborating miniature drones for surveillance and reconnaissance</strong> <div class="csl-block">Axel Bürkle</div> <em>Unmanned/Unattended Sensors and Sensor Networks VI</em> (2009-09) <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/7480/74800H/Collaborating-miniature-drones-for-surveillance-and-reconnaissance/10.1117/12.830408.short">https://www.spiedigitallibrary.org/conference-proceedings-of-spie/7480/74800H/Collaborating-miniature-drones-for-surveillance-and-reconnaissance/10.1117/12.830408.short</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1117/12.830408">10.1117/12.830408</a></div></div>
</div>
<div id="ref-https://apps.dtic.mil/sti/citations/AD1039921" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[Sand21] </div><div class="csl-right-inline"><strong>Drone Swarms</strong> <div class="csl-block">Andrew W Sanders</div> <em>US Army School for Advanced Military Studies Fort Leavenworth United States</em> (2017-05) <a href="https://apps.dtic.mil/sti/citations/AD1039921">https://apps.dtic.mil/sti/citations/AD1039921</a></div>
</div>
<div id="ref-https://ieeexplore.ieee.org/document/7568316" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[CeSp21] </div><div class="csl-right-inline"><strong>Controlling swarms of medical nanorobots using CPPSO on a GPU</strong> <div class="csl-block">Davide Ceraso, Giandomenico Spezzano</div> <em>2016 International Conference on High Performance Computing Simulation (HPCS)</em> (2016-07) <a href="https://ieeexplore.ieee.org/document/7568316">https://ieeexplore.ieee.org/document/7568316</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/HPCSim.2016.7568316">10.1109/hpcsim.2016.7568316</a></div></div>
</div>
<div id="ref-https://link.springer.com/article/10.1007/s00146-018-0845-5" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[TuDe21] </div><div class="csl-right-inline"><strong>Classification of global catastrophic risks connected with artificial intelligence</strong> <div class="csl-block">Alexey Turchin, David Denkenberger</div> <em>AI &amp; SOCIETY</em> (2020-03) <a href="https://doi.org/10.1007/s00146-018-0845-5">https://doi.org/10.1007/s00146-018-0845-5</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s00146-018-0845-5">10.1007/s00146-018-0845-5</a></div></div>
</div>
<div id="ref-https://arxiv.org/abs/1808.00177" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[ABC21] </div><div class="csl-right-inline"><strong>Learning Dexterous In-Hand Manipulation</strong> <div class="csl-block">OpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, … Wojciech Zaremba</div> <em>arXiv:1808.00177 [cs, stat]</em> (2019-01) <a href="http://arxiv.org/abs/1808.00177">http://arxiv.org/abs/1808.00177</a></div>
</div>
<div id="ref-https://arxiv.org/abs/1909.07528" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[BKMW21] </div><div class="csl-right-inline"><strong>Emergent Tool Use From Multi-Agent Autocurricula</strong> <div class="csl-block">Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, Igor Mordatch</div> <em>arXiv:1909.07528 [cs, stat]</em> (2020-02) <a href="http://arxiv.org/abs/1909.07528">http://arxiv.org/abs/1909.07528</a></div>
</div>
<div id="ref-http://proceedings.mlr.press/v37/schulman15.html" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[SLAJ21] </div><div class="csl-right-inline"><strong>Trust Region Policy Optimization</strong> <div class="csl-block">John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz</div> <em>International Conference on Machine Learning</em> (2015-06) <a href="http://proceedings.mlr.press/v37/schulman15.html">http://proceedings.mlr.press/v37/schulman15.html</a></div>
</div>
<div id="ref-https://arxiv.org/abs/1707.06347" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[SWDR21] </div><div class="csl-right-inline"><strong>Proximal Policy Optimization Algorithms</strong> <div class="csl-block">John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov</div> <em>arXiv:1707.06347 [cs]</em> (2017-08) <a href="http://arxiv.org/abs/1707.06347">http://arxiv.org/abs/1707.06347</a></div>
</div>
<div id="ref-https://arxiv.org/abs/1801.01290" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[HZAL21] </div><div class="csl-right-inline"><strong>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</strong> <div class="csl-block">Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine</div> <em>arXiv:1801.01290 [cs, stat]</em> (2018-08) <a href="http://arxiv.org/abs/1801.01290">http://arxiv.org/abs/1801.01290</a></div>
</div>
<div id="ref-https://arxiv.org/abs/1506.02438" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[SMLJ21] </div><div class="csl-right-inline"><strong>High-Dimensional Continuous Control Using Generalized Advantage Estimation</strong> <div class="csl-block">John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel</div> <em>arXiv:1506.02438 [cs]</em> (2018-10) <a href="http://arxiv.org/abs/1506.02438">http://arxiv.org/abs/1506.02438</a></div>
</div>
<div id="ref-https://github.com/DLR-RM/stable-baselines3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[RHEG19] </div><div class="csl-right-inline"><strong>Stable Baselines3</strong> <div class="csl-block">Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Noah Dormann</div> <em>GitHub repository</em> (2019)</div>
</div>
<div id="ref-https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html_x35_results" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[RHEG21] </div><div class="csl-right-inline"><strong>PPO — Stable Baselines3 1.1.0a11 documentation</strong> <div class="csl-block">Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Noah Dormann</div> <em>stable-baselines3.readthedocs.io</em> (2020) <a href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#results">https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#results</a></div>
</div>
<div id="ref-https://openreview.net/forum?id_x61_qYZD-AO1Vn" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[OBNZ21] </div><div class="csl-right-inline"><strong>Differentiable Trust Region Layers for Deep Reinforcement Learning</strong> <div class="csl-block">Fabian Otto, Philipp Becker, Vien Anh Ngo, Hanna Carolin Maria Ziesche, Gerhard Neumann</div> (2020-09) <a href="https://openreview.net/forum?id=qYZD-AO1Vn">https://openreview.net/forum?id=qYZD-AO1Vn</a></div>
</div>
<div id="ref-doi:10.1214/aoms/1177729694" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[KuLe21] </div><div class="csl-right-inline"><strong>On Information and Sufficiency</strong> <div class="csl-block">S Kullback, RA Leibler</div> <em>The Annals of Mathematical Statistics</em> (1951-03) <a href="http://projecteuclid.org/euclid.aoms/1177729694">http://projecteuclid.org/euclid.aoms/1177729694</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1214/aoms/1177729694">10.1214/aoms/1177729694</a></div></div>
</div>
<div id="ref-https://www.springer.com/gp/book/9783540710493" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[Vill21] </div><div class="csl-right-inline"><strong>Optimal Transport: Old and New</strong> <div class="csl-block">Cédric Villani</div> <em>Springer-Verlag</em> (2009) <a href="https://www.springer.com/gp/book/9783540710493">https://www.springer.com/gp/book/9783540710493</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/978-3-540-71050-9">10.1007/978-3-540-71050-9</a> · ISBN: <a href="https://worldcat.org/isbn/9783540710493">9783540710493</a></div></div>
</div>
<div id="ref-https://www.springer.com/gp/book/9783319289274" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[OlAm21] </div><div class="csl-right-inline"><strong>A Concise Introduction to Decentralized POMDPs</strong> <div class="csl-block">Frans A Oliehoek, Christopher Amato</div> <em>Springer International Publishing</em> (2016) <a href="https://www.springer.com/gp/book/9783319289274">https://www.springer.com/gp/book/9783319289274</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/978-3-319-28929-8">10.1007/978-3-319-28929-8</a> · ISBN: <a href="https://worldcat.org/isbn/9783319289274">9783319289274</a></div></div>
</div>
<div id="ref-https://jmlr.org/beta/papers/v20/18-476.html" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[HüŠoNe21] </div><div class="csl-right-inline"><strong>Deep Reinforcement Learning for Swarm Systems</strong> <div class="csl-block">Maximilian Hüttenrauch, Adrian Šošić, Gerhard Neumann</div> <em>Journal of Machine Learning Research</em> (2019) <a href="http://jmlr.org/papers/v20/18-476.html">http://jmlr.org/papers/v20/18-476.html</a></div>
</div>
<div id="ref-https://dl.acm.org/doi/10.5555/3091125.3091320" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[ŠKZK21] </div><div class="csl-right-inline"><strong>Inverse Reinforcement Learning in Swarm Systems</strong> <div class="csl-block">Adrian Šošić, Wasiur R KhudaBukhsh, Abdelhak M Zoubir, Heinz Koeppl</div> <em>Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems</em> (2017-05) <a href="https://dl.acm.org/doi/10.5555/3091125.3091320">https://dl.acm.org/doi/10.5555/3091125.3091320</a></div>
</div>
<div id="ref-https://dl.acm.org/doi/10.5555/3295222.3295385" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[LWTH21] </div><div class="csl-right-inline"><strong>Multi-agent actor-critic for mixed cooperative-competitive environments</strong> <div class="csl-block">Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch</div> <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em> (2017-12) <a href="https://dl.acm.org/doi/10.5555/3295222.3295385">https://dl.acm.org/doi/10.5555/3295222.3295385</a> <div class="csl-block">ISBN: <a href="https://worldcat.org/isbn/9781510860964">9781510860964</a></div></div>
</div>
<div id="ref-https://www.semanticscholar.org/paper/Using-M-Embeddings-to-Learn-Control-Strategies-for-Gebhardt-H_x37_C3_x37_BCttenrauch/9f550815f8858e7c4c8aef23665fa5817884f1b3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[GeHüNe21] </div><div class="csl-right-inline"><strong>Using M-Embeddings to Learn Control Strategies for Robot Swarms</strong> <div class="csl-block">Gregor HW Gebhardt, Maximilian Hüttenrauch, G Neumann</div> <em>www.semanticscholar.org</em> (2019) <a href="https://www.semanticscholar.org/paper/Using-M-Embeddings-to-Learn-Control-Strategies-for-Gebhardt-H%C3%BCttenrauch/9f550815f8858e7c4c8aef23665fa5817884f1b3">https://www.semanticscholar.org/paper/Using-M-Embeddings-to-Learn-Control-Strategies-for-Gebhardt-H%C3%BCttenrauch/9f550815f8858e7c4c8aef23665fa5817884f1b3</a></div>
</div>
<div id="ref-https://link.springer.com/article/10.1007/s11537-007-0657-8" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[LaLi21] </div><div class="csl-right-inline"><strong>Mean field games</strong> <div class="csl-block">Jean-Michel Lasry, Pierre-Louis Lions</div> <em>Japanese Journal of Mathematics</em> (2007-03) <a href="https://doi.org/10.1007/s11537-007-0657-8">https://doi.org/10.1007/s11537-007-0657-8</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s11537-007-0657-8">10.1007/s11537-007-0657-8</a></div></div>
</div>
<div id="ref-http://proceedings.mlr.press/v80/yang18d.html" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[YLLZ21] </div><div class="csl-right-inline"><strong>Mean Field Multi-Agent Reinforcement Learning</strong> <div class="csl-block">Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, Jun Wang</div> <em>International Conference on Machine Learning</em> (2018-07) <a href="http://proceedings.mlr.press/v80/yang18d.html">http://proceedings.mlr.press/v80/yang18d.html</a></div>
</div>
<div id="ref-https://ieeexplore.ieee.org/abstract/document/9137257" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[CQZW21] </div><div class="csl-right-inline"><strong>Mean Field Deep Reinforcement Learning for Fair and Efficient UAV Control</strong> <div class="csl-block">Dezhi Chen, Qi Qi, Zirui Zhuang, Jingyu Wang, Jianxin Liao, Zhu Han</div> <em>IEEE Internet of Things Journal</em> (2021-01) <a href="https://ieeexplore.ieee.org/abstract/document/9137257">https://ieeexplore.ieee.org/abstract/document/9137257</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/JIOT.2020.3008299">10.1109/jiot.2020.3008299</a></div></div>
</div>
<div id="ref-https://arxiv.org/abs/1703.04908" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[MoAb21] </div><div class="csl-right-inline"><strong>Emergence of Grounded Compositional Language in Multi-Agent Populations</strong> <div class="csl-block">Igor Mordatch, Pieter Abbeel</div> <em>arXiv:1703.04908 [cs]</em> (2018-07) <a href="http://arxiv.org/abs/1703.04908">http://arxiv.org/abs/1703.04908</a></div>
</div>
<div id="ref-https://link.springer.com/chapter/10.1007/978-3-540-28650-9_4" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[Rasm21] </div><div class="csl-right-inline"><strong>Gaussian Processes in Machine Learning</strong> <div class="csl-block">Carl Edward Rasmussen</div> <em>Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2 - 14, 2003, Tübingen, Germany, August 4 - 16, 2003, Revised Lectures</em> (2004) <a href="https://doi.org/10.1007/978-3-540-28650-9_4">https://doi.org/10.1007/978-3-540-28650-9_4</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/978-3-540-28650-9_4">10.1007/978-3-540-28650-9_4</a> · ISBN: <a href="https://worldcat.org/isbn/9783540286509">9783540286509</a></div></div>
</div>
<div id="ref-https://openreview.net/forum?id_x61_ufZN2-aehFa" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[VFGD21] </div><div class="csl-right-inline"><strong>Bayesian Context Aggregation for Neural Processes</strong> <div class="csl-block">Michael Volpp, Fabian Flürenbrock, Lukas Grossberger, Christian Daniel, Gerhard Neumann</div> (2020-09) <a href="https://openreview.net/forum?id=ufZN2-aehFa">https://openreview.net/forum?id=ufZN2-aehFa</a></div>
</div>
<div id="ref-https://www.springer.com/gp/book/9780387310732" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[Bish21] </div><div class="csl-right-inline"><strong>Pattern Recognition and Machine Learning</strong> <div class="csl-block">Christopher Bishop</div> <em>Springer-Verlag</em> (2006) <a href="https://www.springer.com/gp/book/9780387310732">https://www.springer.com/gp/book/9780387310732</a> <div class="csl-block">ISBN: <a href="https://worldcat.org/isbn/9780387310732">9780387310732</a></div></div>
</div>
<div id="ref-https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[VSPU21] </div><div class="csl-right-inline"><strong>Attention is All you Need</strong> <div class="csl-block">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin</div> <em>Advances in Neural Information Processing Systems</em> (2017) <a href="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</a></div>
</div>
<div id="ref-http://proceedings.mlr.press/v97/iqbal19a.html" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[IqSh21] </div><div class="csl-right-inline"><strong>Actor-Attention-Critic for Multi-Agent Reinforcement Learning</strong> <div class="csl-block">Shariq Iqbal, Fei Sha</div> <em>International Conference on Machine Learning</em> (2019-05) <a href="http://proceedings.mlr.press/v97/iqbal19a.html">http://proceedings.mlr.press/v97/iqbal19a.html</a></div>
</div>
<div id="ref-https://ieeexplore.ieee.org/document/9049415" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[LiTa21] </div><div class="csl-right-inline"><strong>Attentive Relational State Representation in Decentralized Multiagent Reinforcement Learning</strong> <div class="csl-block">Xiangyu Liu, Ying Tan</div> <em>IEEE Transactions on Cybernetics</em> (2020) <a href="https://ieeexplore.ieee.org/document/9049415">https://ieeexplore.ieee.org/document/9049415</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/TCYB.2020.2979803">10.1109/tcyb.2020.2979803</a></div></div>
</div>
<div id="ref-https://arxiv.org/abs/1911.10635" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[ZhYaBa21] </div><div class="csl-right-inline"><strong>Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms</strong> <div class="csl-block">Kaiqing Zhang, Zhuoran Yang, Tamer Başar</div> <em>arXiv:1911.10635 [cs, stat]</em> (2021-04) <a href="http://arxiv.org/abs/1911.10635">http://arxiv.org/abs/1911.10635</a></div>
</div>
<div id="ref-https://www.mdpi.com/2076-3417/11/11/4948" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[CCDF21] </div><div class="csl-right-inline"><strong>Multi-Agent Reinforcement Learning: A Review of Challenges and Applications</strong> <div class="csl-block">Lorenzo Canese, Gian Carlo Cardarilli, Luca Di Nunzio, Rocco Fazzolari, Daniele Giardino, Marco Re, Sergio Spanò</div> <em>Applied Sciences</em> (2021-01) <a href="https://www.mdpi.com/2076-3417/11/11/4948">https://www.mdpi.com/2076-3417/11/11/4948</a> <div class="csl-block">DOI: <a href="https://doi.org/10.3390/app11114948">10.3390/app11114948</a></div></div>
</div>
<div id="ref-https://arxiv.org/abs/1806.00877" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[WYWH21] </div><div class="csl-right-inline"><strong>Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual Optimization</strong> <div class="csl-block">Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, Mingyi Hong</div> <em>arXiv:1806.00877 [cs, math, stat]</em> (2019-01) <a href="http://arxiv.org/abs/1806.00877">http://arxiv.org/abs/1806.00877</a></div>
</div>
<div id="ref-https://ieeexplore.ieee.org/document/6415291" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[KaMoPo13] </div><div class="csl-right-inline"><strong>Q D-learning: A collaborative distributed strategy for multi-agent reinforcement learning through {\rm Consensus} + {\rm Innovations}</strong> <div class="csl-block">Soummya Kar, José MF Moura, HVincent Poor</div> <em>IEEE Transactions on Signal Processing</em> (2013) <div class="csl-block">DOI: <a href="https://doi.org/10.1109/TSP.2013.2241057">10.1109/tsp.2013.2241057</a></div></div>
</div>
<div id="ref-http://proceedings.mlr.press/v80/zhang18n.html" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[ZYLZ21] </div><div class="csl-right-inline"><strong>Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents</strong> <div class="csl-block">Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, Tamer Basar</div> <em>International Conference on Machine Learning</em> (2018-07) <a href="http://proceedings.mlr.press/v80/zhang18n.html">http://proceedings.mlr.press/v80/zhang18n.html</a></div>
</div>
<div id="ref-https://arxiv.org/abs/1705.08926" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[FFAN21] </div><div class="csl-right-inline"><strong>Counterfactual Multi-Agent Policy Gradients</strong> <div class="csl-block">Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, Shimon Whiteson</div> <em>arXiv:1705.08926 [cs]</em> (2017-12) <a href="http://arxiv.org/abs/1705.08926">http://arxiv.org/abs/1705.08926</a></div>
</div>
<div id="ref-https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[GuEgKo21] </div><div class="csl-right-inline"><strong>Cooperative Multi-agent Control Using Deep Reinforcement Learning</strong> <div class="csl-block">Jayesh K Gupta, Maxim Egorov, Mykel Kochenderfer</div> <em>Autonomous Agents and Multiagent Systems</em> (2017) <a href="https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5">https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/978-3-319-71682-4_5">10.1007/978-3-319-71682-4_5</a> · ISBN: <a href="https://worldcat.org/isbn/9783319716824">9783319716824</a></div></div>
</div>
<div id="ref-https://link.springer.com/chapter/10.1007/978-3-642-27645-3_15" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[Olie21] </div><div class="csl-right-inline"><strong>Decentralized POMDPs</strong> <div class="csl-block">Frans A Oliehoek</div> <em>Reinforcement Learning: State-of-the-Art</em> (2012) <a href="https://doi.org/10.1007/978-3-642-27645-3_15">https://doi.org/10.1007/978-3-642-27645-3_15</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/978-3-642-27645-3_15">10.1007/978-3-642-27645-3_15</a> · ISBN: <a href="https://worldcat.org/isbn/9783642276453">9783642276453</a></div></div>
</div>
<div id="ref-https://proceedings.neurips.cc/paper/2016/hash/c7635bfd99248a2cdef8249ef7bfbef4-Abstract.html" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[FAFW21] </div><div class="csl-right-inline"><strong>Learning to Communicate with Deep Multi-Agent Reinforcement Learning</strong> <div class="csl-block">Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, Shimon Whiteson</div> <em>Advances in Neural Information Processing Systems</em> (2016) <a href="https://proceedings.neurips.cc/paper/2016/hash/c7635bfd99248a2cdef8249ef7bfbef4-Abstract.html">https://proceedings.neurips.cc/paper/2016/hash/c7635bfd99248a2cdef8249ef7bfbef4-Abstract.html</a></div>
</div>
<div id="ref-https://ieeexplore.ieee.org/document/4399095" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[MaLaLe21] </div><div class="csl-right-inline"><strong>Hysteretic Q-learning : An algorithm for Decentralized Reinforcement Learning in Cooperative Multi-Agent Teams</strong> <div class="csl-block">Laetitia Matignon, Guillaume J Laurent, Nadine Le Fort-Piat</div> <em>2007 IEEE/RSJ International Conference on Intelligent Robots and Systems</em> (2007-10) <a href="https://ieeexplore.ieee.org/document/4399095">https://ieeexplore.ieee.org/document/4399095</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/IROS.2007.4399095">10.1109/iros.2007.4399095</a></div></div>
</div>
<div id="ref-https://ojs.aaai.org/index.php/AAAI/article/view/11371" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[ZYCZ21] </div><div class="csl-right-inline"><strong>MAgent: A Many-Agent Reinforcement Learning Platform for Artificial Collective Intelligence</strong> <div class="csl-block">Lianmin Zheng, Jiacheng Yang, Han Cai, Ming Zhou, Weinan Zhang, Jun Wang, Yong Yu</div> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (2018-04) <a href="https://ojs.aaai.org/index.php/AAAI/article/view/11371">https://ojs.aaai.org/index.php/AAAI/article/view/11371</a></div>
</div>
<div id="ref-https://www.nature.com/articles/nature24270" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[SSSA21] </div><div class="csl-right-inline"><strong>Mastering the game of Go without human knowledge</strong> <div class="csl-block">David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, … Demis Hassabis</div> <em>Nature</em> (2017-10) <a href="https://www.nature.com/articles/nature24270">https://www.nature.com/articles/nature24270</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nature24270">10.1038/nature24270</a></div></div>
</div>
<div id="ref-https://arxiv.org/abs/2009.14471" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[TBGJ21] </div><div class="csl-right-inline"><strong>PettingZoo: Gym for Multi-Agent Reinforcement Learning</strong> <div class="csl-block">JK Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth Hari, Ryan Sullivan, Luis Santos, Rodrigo Perez, Caroline Horsch, Clemens Dieffendahl, … Praveen Ravi</div> <em>arXiv:2009.14471 [cs, stat]</em> (2021-06) <a href="http://arxiv.org/abs/2009.14471">http://arxiv.org/abs/2009.14471</a></div>
</div>
<div id="ref-https://dl.acm.org/doi/10.1145/3292500.3330701" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[ASYO21] </div><div class="csl-right-inline"><strong>Optuna: A Next-generation Hyperparameter Optimization Framework</strong> <div class="csl-block">Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, Masanori Koyama</div> <em>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> (2019-07) <a href="https://doi.org/10.1145/3292500.3330701">https://doi.org/10.1145/3292500.3330701</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/3292500.3330701">10.1145/3292500.3330701</a> · ISBN: <a href="https://worldcat.org/isbn/9781450362016">9781450362016</a></div></div>
</div>
<div id="ref-https://ieeexplore.ieee.org/document/976029" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[EgHu21] </div><div class="csl-right-inline"><strong>Formation constrained multi-agent control</strong> <div class="csl-block">M Egerstedt, Xiaoming Hu</div> <em>IEEE Transactions on Robotics and Automation</em> (2001-12) <a href="https://ieeexplore.ieee.org/document/976029">https://ieeexplore.ieee.org/document/976029</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/70.976029">10.1109/70.976029</a></div></div>
</div>
<div id="ref-https://www.sciencedirect.com/science/article/abs/pii/S0005109816301911" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[ZZDH21] </div><div class="csl-right-inline"><strong>Cooperative pursuit with Voronoi partitions</strong> <div class="csl-block">Zhengyuan Zhou, Wei Zhang, Jerry Ding, Haomiao Huang, Dušan M Stipanović, Claire J Tomlin</div> <em>Automatica</em> (2016-10) <a href="https://www.sciencedirect.com/science/article/pii/S0005109816301911">https://www.sciencedirect.com/science/article/pii/S0005109816301911</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.automatica.2016.05.007">10.1016/j.automatica.2016.05.007</a></div></div>
</div>
</div>
<!-- default theme -->

<style>
  /* import google fonts */
  @import url("https://fonts.googleapis.com/css?family=Open+Sans:400,600,700");
  @import url("https://fonts.googleapis.com/css?family=Source+Code+Pro");

  /* -------------------------------------------------- */
  /* global */
  /* -------------------------------------------------- */

  /* all elements */
  * {
    /* force sans-serif font unless specified otherwise */
    font-family: "Open Sans", "Helvetica", sans-serif;

    /* prevent text inflation on some mobile browsers */
    -webkit-text-size-adjust: none !important;
    -moz-text-size-adjust: none !important;
    -o-text-size-adjust: none !important;
    text-size-adjust: none !important;
  }

  @media only screen {
    /* "page" element */
    body {
      position: relative;
      box-sizing: border-box;
      font-size: 12pt;
      line-height: 1.5;
      max-width: 8.5in;
      margin: 20px auto;
      padding: 40px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* "page" element */
    body {
      padding: 20px;
      margin: 0;
      border-radius: 0;
      border: none;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05) inset;
      background: none;
    }
  }

  /* -------------------------------------------------- */
  /* headings */
  /* -------------------------------------------------- */

  /* all headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    margin: 20px 0;
    padding: 0;
    font-weight: bold;
  }

  /* biggest heading */
  h1 {
    margin: 40px 0;
    text-align: center;
  }

  /* second biggest heading */
  h2 {
    margin-top: 30px;
    padding-bottom: 5px;
    border-bottom: solid 1px #bdbdbd;
  }

  /* heading font sizes */
  h1 {
    font-size: 2em;
  }
  h2 {
    font-size: 1.5em;
  }
  h3 {
    font-size: 1.35em;
  }
  h4 {
    font-size: 1.25em;
  }
  h5 {
    font-size: 1.15em;
  }
  h6 {
    font-size: 1em;
  }

  /* -------------------------------------------------- */
  /* manuscript header */
  /* -------------------------------------------------- */

  /* manuscript title */
  header > h1 {
    margin: 0;
  }

  /* manuscript title caption text (ie "automatically generated on") */
  header + p {
    text-align: center;
    margin-top: 10px;
  }

  /* -------------------------------------------------- */
  /* text elements */
  /* -------------------------------------------------- */

  /* links */
  a {
    color: #2196f3;
    overflow-wrap: break-word;
  }

  /* superscripts and subscripts */
  sub,
  sup {
    /* prevent from affecting line height */
    line-height: 0;
  }

  /* unordered and ordered lists*/
  ul,
  ol {
    padding-left: 20px;
  }

  /* class for styling text semibold */
  .semibold {
    font-weight: 600;
  }

  /* class for styling elements horizontally left aligned */
  .left {
    display: block;
    text-align: left;
    margin-left: auto;
    margin-right: 0;
    justify-content: left;
  }

  /* class for styling elements horizontally centered */
  .center {
    display: block;
    text-align: center;
    margin-left: auto;
    margin-right: auto;
    justify-content: center;
  }

  /* class for styling elements horizontally right aligned */
  .right {
    display: block;
    text-align: right;
    margin-left: 0;
    margin-right: auto;
    justify-content: right;
  }

  /* -------------------------------------------------- */
  /* section elements */
  /* -------------------------------------------------- */

  /* horizontal divider line */
  hr {
    border: none;
    height: 1px;
    background: #bdbdbd;
  }

  /* paragraphs, horizontal dividers, figures, tables, code */
  p,
  hr,
  figure,
  table,
  pre {
    /* treat all as "paragraphs", with consistent vertical margins */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* figures */
  /* -------------------------------------------------- */

  /* figure */
  figure {
    max-width: 100%;
    margin-left: auto;
    margin-right: auto;
    padding: 1ex;
    border: 1px solid gray;
  }

  /* figure caption */
  figcaption {
    padding: 0;
    padding-top: 10px;
    font-style: italic;
  }

  /* figure image element */
  figure > img,
  figure > svg {
    max-width: 100%;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure auto-number */
  img + figcaption > span:first-of-type,
  svg + figcaption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* tables */
  /* -------------------------------------------------- */

  /* table */
  table {
    border-collapse: collapse;
    border-spacing: 0;
    width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* table cells */
  th,
  td {
    border: solid 1px #bdbdbd;
    padding: 10px;
    /* squash table if too wide for page by forcing line breaks */
    overflow-wrap: break-word;
    word-break: break-word;
  }

  /* header row and even rows */
  th,
  tr:nth-child(2n) {
    background-color: #fafafa;
  }

  /* odd rows */
  tr:nth-child(2n + 1) {
    background-color: #ffffff;
  }

  /* table caption */
  caption {
    text-align: left;
    padding: 0;
    padding-bottom: 10px;
  }

  /* table auto-number */
  table > caption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* code */
  /* -------------------------------------------------- */

  /* multi-line code block */
  pre {
    padding: 10px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
    break-inside: avoid;
    text-align: left;
  }

  /* inline code, ie code within normal text */
  :not(pre) > code {
    padding: 0 4px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
  }

  /* code text */
  /* apply all children, to reach syntax highlighting sub-elements */
  code,
  code * {
    /* force monospace font */
    font-family: "Source Code Pro", "Courier New", monospace;
  }

  /* -------------------------------------------------- */
  /* quotes */
  /* -------------------------------------------------- */

  /* quoted text */
  blockquote {
    margin: 0;
    padding: 0;
    border-left: 4px solid #bdbdbd;
    padding-left: 16px;
    break-inside: avoid;
  }

  /* -------------------------------------------------- */
  /* banners */
  /* -------------------------------------------------- */

  /* info banners */
  .banner {
    box-sizing: border-box;
    display: block;
    position: relative;
    width: 100%;
    margin-top: 20px;
    margin-bottom: 20px;
    padding: 20px;
    text-align: center;
  }

  /* paragraph in banner */
  .banner > p {
    margin: 0;
  }

  /* -------------------------------------------------- */
  /* highlight colors */
  /* -------------------------------------------------- */

  .white {
    background: #ffffff;
  }
  .lightgrey {
    background: #eeeeee;
  }
  .grey {
    background: #757575;
  }
  .darkgrey {
    background: #424242;
  }
  .black {
    background: #000000;
  }
  .lightred {
    background: #ffcdd2;
  }
  .lightyellow {
    background: #ffecb3;
  }
  .lightgreen {
    background: #dcedc8;
  }
  .lightblue {
    background: #e3f2fd;
  }
  .lightpurple {
    background: #f3e5f5;
  }
  .red {
    background: #f44336;
  }
  .orange {
    background: #ff9800;
  }
  .yellow {
    background: #ffeb3b;
  }
  .green {
    background: #4caf50;
  }
  .blue {
    background: #2196f3;
  }
  .purple {
    background: #9c27b0;
  }
  .white,
  .lightgrey,
  .lightred,
  .lightyellow,
  .lightgreen,
  .lightblue,
  .lightpurple,
  .orange,
  .yellow,
  .white a,
  .lightgrey a,
  .lightred a,
  .lightyellow a,
  .lightgreen a,
  .lightblue a,
  .lightpurple a,
  .orange a,
  .yellow a {
    color: #000000;
  }
  .grey,
  .darkgrey,
  .black,
  .red,
  .green,
  .blue,
  .purple,
  .grey a,
  .darkgrey a,
  .black a,
  .red a,
  .green a,
  .blue a,
  .purple a {
    color: #ffffff;
  }

  /* -------------------------------------------------- */
  /* buttons */
  /* -------------------------------------------------- */

  /* class for styling links like buttons */
  .button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    margin: 5px;
    padding: 10px 20px;
    font-size: 0.75em;
    font-weight: 600;
    text-transform: uppercase;
    text-decoration: none;
    letter-spacing: 1px;
    background: none;
    color: #2196f3;
    border: solid 1px #bdbdbd;
    border-radius: 5px;
  }

  /* buttons when hovered */
  .button:hover:not([disabled]),
  .icon_button:hover:not([disabled]) {
    cursor: pointer;
    background: #f5f5f5;
  }

  /* buttons when disabled */
  .button[disabled],
  .icon_button[disabled] {
    opacity: 0.35;
    pointer-events: none;
  }

  /* class for styling buttons containg only single icon */
  .icon_button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    text-decoration: none;
    margin: 0;
    padding: 0;
    background: none;
    border-radius: 5px;
    border: none;
    width: 20px;
    height: 20px;
    min-width: 20px;
    min-height: 20px;
  }

  /* icon button inner svg image */
  .icon_button > svg {
    height: 16px;
  }

  /* -------------------------------------------------- */
  /* icons */
  /* -------------------------------------------------- */

  /* class for styling icons inline with text */
  .inline_icon {
    height: 1em;
    position: relative;
    top: 0.125em;
  }

  /* -------------------------------------------------- */
  /* references */
  /* -------------------------------------------------- */

  .csl-entry {
    margin-top: 15px;
    margin-bottom: 15px;
  }

  /* -------------------------------------------------- */
  /* print control */
  /* -------------------------------------------------- */

  @media print {
    @page {
      /* suggested printing margin */
      margin: 0.5in;
    }

    /* document and "page" elements */
    html,
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
    }

    /* "page" element */
    body {
      font-size: 11pt !important;
      line-height: 1.35;
    }

    /* all headings */
    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin: 15px 0;
    }

    /* figures and tables */
    figure,
    table {
      font-size: 0.85em;
    }

    /* table cells */
    th,
    td {
      padding: 5px;
    }

    /* shrink font awesome icons */
    i.fas,
    i.fab,
    i.far,
    i.fal {
      transform: scale(0.85);
    }

    /* decrease banner margins */
    .banner {
      margin-top: 15px;
      margin-bottom: 15px;
      padding: 15px;
    }

    /* class for centering an element vertically on its own page */
    .page_center {
      margin: auto;
      width: 100%;
      height: 100%;
      display: flex;
      align-items: center;
      vertical-align: middle;
      break-before: page;
      break-after: page;
    }

    /* always insert a page break before the element */
    .page_break_before {
      break-before: page;
    }

    /* always insert a page break after the element */
    .page_break_after {
      break-after: page;
    }

    /* avoid page break before the element */
    .page_break_before_avoid {
      break-before: avoid;
    }

    /* avoid page break after the element */
    .page_break_after_avoid {
      break-after: avoid;
    }

    /* avoid page break inside the element */
    .page_break_inside_avoid {
      break-inside: avoid;
    }
  }

  /* -------------------------------------------------- */
  /* override pandoc css quirks */
  /* -------------------------------------------------- */

  .sourceCode {
    /* prevent unsightly overflow in wide code blocks */
    overflow: auto !important;
  }

  div.sourceCode {
    /* prevent background fill on top-most code block  container */
    background: none !important;
  }

  .sourceCode * {
    /* force consistent line spacing */
    line-height: 1.5 !important;
  }

  div.sourceCode {
    /* style code block margins same as <pre> element */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* tablenos */
  /* -------------------------------------------------- */

  /* tablenos wrapper */
  .tablenos {
    width: 100%;
    margin: 20px 0;
  }

  .tablenos > table {
    /* move margins from table to table_wrapper to allow margin collapsing */
    margin: 0;
  }

  @media only screen {
    /* tablenos wrapper */
    .tablenos {
      /* show scrollbar on tables if necessary to prevent overflow */
      overflow-x: auto !important;
    }

    .tablenos th,
    .tablenos td {
      overflow-wrap: unset !important;
      word-break: unset !important;
    }

    /* table in wrapper */
    .tablenos table,
    .tablenos table * {
      /* don't break table words */
      overflow-wrap: normal !important;
    }
  }
</style>
<!-- 
    Plugin Core

    Functions needed for and shared across all first-party plugins.
-->

<script>
  // get element that is target of hash (from link element or url)
  function getHashTarget(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector(`[id="${id}"]`);
    if (!target) return;

    // if figure or table, modify target to get expected element
    if (id.indexOf("fig:") === 0) target = target.querySelector("figure");
    if (id.indexOf("tbl:") === 0) target = target.querySelector("table");

    return target;
  }

  // get position/dimensions of element or viewport
  function getRectInView(element) {
    let rect = {};
    rect.left = 0;
    rect.top = 0;
    rect.right = document.documentElement.clientWidth;
    rect.bottom = document.documentElement.clientHeight;
    let style = {};

    if (element instanceof HTMLElement) {
      rect = element.getBoundingClientRect();
      style = window.getComputedStyle(element);
    }

    const margin = {};
    margin.left = parseFloat(style.marginLeftWidth) || 0;
    margin.top = parseFloat(style.marginTopWidth) || 0;
    margin.right = parseFloat(style.marginRightWidth) || 0;
    margin.bottom = parseFloat(style.marginBottomWidth) || 0;

    const border = {};
    border.left = parseFloat(style.borderLeftWidth) || 0;
    border.top = parseFloat(style.borderTopWidth) || 0;
    border.right = parseFloat(style.borderRightWidth) || 0;
    border.bottom = parseFloat(style.borderBottomWidth) || 0;

    const newRect = {};
    newRect.left = rect.left + margin.left + border.left;
    newRect.top = rect.top + margin.top + border.top;
    newRect.right = rect.right + margin.right + border.right;
    newRect.bottom = rect.bottom + margin.bottom + border.bottom;
    newRect.width = newRect.right - newRect.left;
    newRect.height = newRect.bottom - newRect.top;

    return newRect;
  }

  // get position of element relative to page
  function getRectInPage(element) {
    const rect = getRectInView(element);
    const body = getRectInView(document.body);

    const newRect = {};
    newRect.left = rect.left - body.left;
    newRect.top = rect.top - body.top;
    newRect.right = rect.right - body.left;
    newRect.bottom = rect.bottom - body.top;
    newRect.width = rect.width;
    newRect.height = rect.height;

    return newRect;
  }

  // get closest element before specified element that matches query
  function firstBefore(element, query) {
    while (element && element !== document.body && !element.matches(query))
      element = element.previousElementSibling || element.parentNode;

    return element;
  }

  // check if element is part of collapsed heading
  function isCollapsed(element) {
    while (element && element !== document.body) {
      if (element.dataset.collapsed === "true") return true;
      element = element.parentNode;
    }
    return false;
  }

  // expand any collapsed parent containers of element if necessary
  function expandElement(element) {
    if (isCollapsed(element)) {
      // accordion plugin
      const heading = firstBefore(element, "h2");
      if (heading) heading.click();
      // details/summary HTML element
      const summary = firstBefore(element, "summary");
      if (summary) summary.click();
    }
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);

    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // get list of elements after a start element up to element matching query
  function nextUntil(element, query, exclude) {
    const elements = [];
    while (((element = element.nextElementSibling), element)) {
      if (element.matches(query)) break;
      if (!element.matches(exclude)) elements.push(element);
    }
    return elements;
  }
</script>
<!--
  Accordion Plugin

  Allows sections of content under h2 headings to be collapsible.
-->

<script type="module">
  // whether to always start expanded ('false'), always start collapsed
  // ('true'), or start collapsed when screen small ('auto')
  const startCollapsed = "auto";

  // start script
  function start() {
    // run through each <h2> heading
    const headings = document.querySelectorAll("h2");
    for (const heading of headings) {
      addArrow(heading);

      // start expanded/collapsed based on option
      if (
        startCollapsed === "true" ||
        (startCollapsed === "auto" && isSmallScreen())
      )
        collapseHeading(heading);
      else expandElement(heading);
    }

    // attach hash change listener to window
    window.addEventListener("hashchange", onHashChange);
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) goToElement(target);
  }

  // add arrow to heading
  function addArrow(heading) {
    // add arrow button
    const arrow = document.createElement("button");
    arrow.innerHTML = document.querySelector(".icon_angle_down").innerHTML;
    arrow.classList.add("icon_button", "accordion_arrow");
    heading.insertBefore(arrow, heading.firstChild);

    // attach click listener to heading and button
    heading.addEventListener("click", onHeadingClick);
    arrow.addEventListener("click", onArrowClick);
  }

  // determine if on mobile-like device with small screen
  function isSmallScreen() {
    return Math.min(window.innerWidth, window.innerHeight) < 480;
  }

  // when <h2> heading is clicked
  function onHeadingClick(event) {
    // only collapse if <h2> itself is target of click (eg, user did
    // not click on anchor within <h2>)
    if (event.target === this) toggleCollapse(this);
  }

  // when arrow button is clicked
  function onArrowClick() {
    toggleCollapse(this.parentNode);
  }

  // collapse section if expanded, expand if collapsed
  function toggleCollapse(heading) {
    if (heading.dataset.collapsed === "false") collapseHeading(heading);
    else expandElement(heading);
  }

  // elements to exclude from collapse, such as table of contents panel,
  // hypothesis panel, etc
  const exclude = "#toc_panel, div.annotator-frame, #lightbox_overlay";

  // collapse section
  function collapseHeading(heading) {
    heading.setAttribute("data-collapsed", "true");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "true");
  }

  // expand section
  function expandElement(heading) {
    heading.setAttribute("data-collapsed", "false");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "false");
  }

  // get list of elements between this <h2> and next <h2> or <h1>
  // ("children" of the <h2> section)
  function getChildren(heading) {
    return nextUntil(heading, "h2, h1", exclude);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle down icon -->

<template class="icon_angle_down">
  <!-- modified from: https://fontawesome.com/icons/angle-down -->
  <svg width="16" height="16" viewBox="0 0 448 512">
    <path
      fill="currentColor"
      d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* accordion arrow button */
    .accordion_arrow {
      margin-right: 10px;
    }

    /* arrow icon when <h2> data-collapsed attribute true */
    h2[data-collapsed="true"] > .accordion_arrow > svg {
      transform: rotate(-90deg);
    }

    /* all elements (except <h2>'s) when data-collapsed attribute true */
    *:not(h2)[data-collapsed="true"] {
      display: none;
    }

    /* accordion arrow button when hovered and <h2>'s when hovered */
    .accordion_arrow:hover,
    h2[data-collapsed="true"]:hover,
    h2[data-collapsed="false"]:hover {
      cursor: pointer;
    }
  }

  /* always hide accordion arrow button on print */
  @media only print {
    .accordion_arrow {
      display: none;
    }
  }
</style>
<!--
  Anchors Plugin

  Adds an anchor next to each of a certain type of element that provides a
  human-readable url to that specific item/position in the document (e.g.
  "manuscript.html#abstract"). It also makes it such that scrolling out of view
  of a target removes its identifier from the url.
-->

<script type="module">
  // which types of elements to add anchors next to, in "document.querySelector"
  // format
  const typesQuery =
    'h1, h2, h3, div[id^="fig:"], div[id^="tbl:"], span[id^="eq:"]';

  // start script
  function start() {
    // add anchor to each element of specified types
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) addAnchor(element);

    // attach scroll listener to window
    window.addEventListener("scroll", onScroll);
  }

  // when window is scrolled
  function onScroll() {
    // if url has hash and user has scrolled out of view of hash
    // target, remove hash from url
    const tolerance = 100;
    const target = getHashTarget();
    if (target) {
      if (
        target.getBoundingClientRect().top > window.innerHeight + tolerance ||
        target.getBoundingClientRect().bottom < 0 - tolerance
      )
        history.pushState(null, null, " ");
    }
  }

  // add anchor to element
  function addAnchor(element) {
    let addTo; // element to add anchor button to

    // if figure or table, modify withId and addTo to get expected
    // elements
    if (element.id.indexOf("fig:") === 0) {
      addTo = element.querySelector("figcaption");
    } else if (element.id.indexOf("tbl:") === 0) {
      addTo = element.querySelector("caption");
    } else if (element.id.indexOf("eq:") === 0) {
      addTo = element.querySelector(".eqnos-number");
    }

    addTo = addTo || element;
    const id = element.id || null;

    // do not add anchor if element doesn't have assigned id.
    // id is generated by pandoc and is assumed to be unique and
    // human-readable
    if (!id) return;

    // create anchor button
    const anchor = document.createElement("a");
    anchor.innerHTML = document.querySelector(".icon_link").innerHTML;
    anchor.title = "Link to this part of the document";
    anchor.classList.add("icon_button", "anchor");
    anchor.dataset.ignore = "true";
    anchor.href = "#" + id;
    addTo.appendChild(anchor);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- link icon -->

<template class="icon_link">
  <!-- modified from: https://fontawesome.com/icons/link -->
  <svg width="16" height="16" viewBox="0 0 512 512">
    <path
      fill="currentColor"
      d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* anchor button */
    .anchor {
      opacity: 0;
      margin-left: 5px;
    }

    /* anchor buttons within <h2>'s */
    h2 .anchor {
      margin-left: 10px;
    }

    /* anchor buttons when hovered/focused and anything containing an anchor button when hovered */
    *:hover > .anchor,
    .anchor:hover,
    .anchor:focus {
      opacity: 1;
    }

    /* anchor button when hovered */
    .anchor:hover {
      cursor: pointer;
    }
  }

  /* always show anchor button on devices with no mouse/hover ability */
  @media (hover: none) {
    .anchor {
      opacity: 1;
    }
  }

  /* always hide anchor button on print */
  @media only print {
    .anchor {
      display: none;
    }
  }
</style>
<!-- 
    Attributes Plugin

    Allows arbitrary HTML attributes to be attached to (almost) any element.
    Place an HTML comment inside or next to the desired element with the content:
    $attribute="value"
-->

<script type="module">
  // start script
  function start() {
    // get list of comments in document
    const comments = findComments();

    for (const comment of comments)
      if (comment.parentElement)
        addAttributes(comment.parentElement, comment.nodeValue.trim());
  }

  // add html attributes to specified element based on string of
  // html attributes and values
  function addAttributes(element, text) {
    // regex's for finding attribute/value pairs in the format of
    // attribute="value" or attribute='value
    const regex2 = /\$([a-zA-Z\-]+)?=\"(.+?)\"/;
    const regex1 = /\$([a-zA-Z\-]+)?=\'(.+?)\'/;

    // loop through attribute/value pairs
    let match;
    while ((match = text.match(regex2) || text.match(regex1))) {
      // get attribute and value from regex capture groups
      let attribute = match[1];
      let value = match[2];

      // remove from string
      text = text.substring(match.index + match[0].length);

      if (!attribute || !value) break;

      // set attribute of parent element
      try {
        element.setAttribute(attribute, value);
      } catch (error) {
        console.log(error);
      }

      // special case for colspan
      if (attribute === "colspan") removeTableCells(element, value);
    }
  }

  // get list of comment elements in document
  function findComments() {
    const comments = [];

    // iterate over comment nodes in document
    function acceptNode(node) {
      return NodeFilter.FILTER_ACCEPT;
    }
    const iterator = document.createNodeIterator(
      document.body,
      NodeFilter.SHOW_COMMENT,
      acceptNode
    );
    let node;
    while ((node = iterator.nextNode())) comments.push(node);

    return comments;
  }

  // remove certain number of cells after specified cell
  function removeTableCells(cell, number) {
    number = parseInt(number);
    if (!number) return;

    // remove elements
    for (; number > 1; number--) {
      if (cell.nextElementSibling) cell.nextElementSibling.remove();
    }
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>
<!--
  Jump to First Plugin

  Adds a button next to each reference entry, figure, and table that jumps the
  page to the first occurrence of a link to that item in the manuscript.
-->

<script type="module">
  // whether to add buttons next to reference entries
  const references = "true";
  // whether to add buttons next to figures
  const figures = "true";
  // whether to add buttons next to tables
  const tables = "true";

  // start script
  function start() {
    if (references !== "false")
      makeButtons(`div[id^="ref-"]`, ".csl-left-margin", "reference");
    if (figures !== "false")
      makeButtons(`div[id^="fig:"]`, "figcaption", "figure");
    if (tables !== "false") makeButtons(`div[id^="tbl:"]`, "caption", "table");
  }

  // when jump button clicked
  function onButtonClick() {
    const first = getFirstOccurrence(this.dataset.id);
    if (!first) return;

    // update url hash so navigating "back" in history will return user to button
    window.location.hash = this.dataset.id;
    // scroll to link
    const timeout = function () {
      goToElement(first, window.innerHeight * 0.5);
    };
    window.setTimeout(timeout, 0);
  }

  // get first occurrence of link to item in document
  function getFirstOccurrence(id) {
    let query = "a";
    query += '[href="#' + id + '"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelector(query);
  }

  // add button next to each reference entry, figure, or table
  function makeButtons(query, containerQuery, subject) {
    const elements = document.querySelectorAll(query);
    for (const element of elements) {
      const id = element.id;
      const buttonContainer = element.querySelector(containerQuery);
      const first = getFirstOccurrence(id);

      // if can't find link to reference or place to put button, ignore
      if (!first || !buttonContainer) continue;

      // make jump button
      let button = document.createElement("button");
      button.classList.add("icon_button", "jump_arrow");
      button.title = `Jump to the first occurrence of this ${subject} in the document`;
      const icon = document.querySelector(".icon_angle_double_up");
      button.innerHTML = icon.innerHTML;
      button.dataset.id = id;
      button.dataset.ignore = "true";
      button.addEventListener("click", onButtonClick);
      buttonContainer.prepend(button);
    }
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle double up icon -->

<template class="icon_angle_double_up">
  <!-- modified from: https://fontawesome.com/icons/angle-double-up -->
  <svg width="16" height="16" viewBox="0 0 320 512">
    <path
      fill="currentColor"
      d="M177 255.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 351.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 425.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1zm-34-192L7 199.7c-9.4 9.4-9.4 24.6 0 33.9l22.6 22.6c9.4 9.4 24.6 9.4 33.9 0l96.4-96.4 96.4 96.4c9.4 9.4 24.6 9.4 33.9 0l22.6-22.6c9.4-9.4 9.4-24.6 0-33.9l-136-136c-9.2-9.4-24.4-9.4-33.8 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* jump button */
    .jump_arrow {
      position: relative;
      top: 0.125em;
      margin-right: 5px;
    }
  }

  /* always hide jump button on print */
  @media only print {
    .jump_arrow {
      display: none;
    }
  }
</style>
<!-- 
    Lightbox Plugin

    Makes it such that when a user clicks on an image, the image fills the
    screen and the user can pan/drag/zoom the image and navigate between other
    images in the document.
-->

<script type="module">
  // list of possible zoom/scale factors
  const zooms =
    "0.1, 0.25, 0.333333, 0.5, 0.666666, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.5, 3, 3.5, 4, 5, 6, 7, 8";
  // whether to fit image to view ('fit'), display at 100% and shrink if
  // necessary ('shrink'), or always display at 100% ('100')
  const defaultZoom = "fit";
  // whether to zoom in/out toward center of view ('true') or mouse ('false')
  const centerZoom = "false";

  // start script
  function start() {
    // run through each <img> element
    const imgs = document.querySelectorAll("figure > img");
    let count = 1;
    for (const img of imgs) {
      img.classList.add("lightbox_document_img");
      img.dataset.number = count;
      img.dataset.total = imgs.length;
      img.addEventListener("click", openLightbox);
      count++;
    }

    // attach mouse and key listeners to window
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("keyup", onKeyUp);
  }

  // when mouse is moved anywhere in window
  function onWindowMouseMove(event) {
    window.mouseX = event.clientX;
    window.mouseY = event.clientY;
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("lightbox_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("lightbox_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeLightbox();
        break;
    }
  }

  // open lightbox
  function openLightbox() {
    const lightbox = makeLightbox(this);
    if (!lightbox) return;

    blurBody(lightbox);
    document.body.appendChild(lightbox);
  }

  // make lightbox
  function makeLightbox(img) {
    // delete lightbox if it exists, start fresh
    closeLightbox();

    // create screen overlay containing lightbox
    const overlay = document.createElement("div");
    overlay.id = "lightbox_overlay";

    // create image info boxes
    const numberInfo = document.createElement("div");
    const zoomInfo = document.createElement("div");
    numberInfo.id = "lightbox_number_info";
    zoomInfo.id = "lightbox_zoom_info";

    // create container for image
    const imageContainer = document.createElement("div");
    imageContainer.id = "lightbox_image_container";
    const lightboxImg = makeLightboxImg(
      img,
      imageContainer,
      numberInfo,
      zoomInfo
    );
    imageContainer.appendChild(lightboxImg);

    // create bottom container for caption and navigation buttons
    const bottomContainer = document.createElement("div");
    bottomContainer.id = "lightbox_bottom_container";
    const caption = makeCaption(img);
    const prevButton = makePrevButton(img);
    const nextButton = makeNextButton(img);
    bottomContainer.appendChild(prevButton);
    bottomContainer.appendChild(caption);
    bottomContainer.appendChild(nextButton);

    // attach top middle and bottom to overlay
    overlay.appendChild(numberInfo);
    overlay.appendChild(zoomInfo);
    overlay.appendChild(imageContainer);
    overlay.appendChild(bottomContainer);

    return overlay;
  }

  // make <img> object that is intuitively draggable and zoomable
  function makeLightboxImg(sourceImg, container, numberInfoBox, zoomInfoBox) {
    // create copy of source <img>
    const img = sourceImg.cloneNode(true);
    img.classList.remove("lightbox_document_img");
    img.removeAttribute("id");
    img.removeAttribute("width");
    img.removeAttribute("height");
    img.style.position = "unset";
    img.style.margin = "0";
    img.style.padding = "0";
    img.style.width = "";
    img.style.height = "";
    img.style.minWidth = "";
    img.style.minHeight = "";
    img.style.maxWidth = "";
    img.style.maxHeight = "";
    img.id = "lightbox_img";

    // build sorted list of zoomSteps
    const zoomSteps = zooms.split(/[^0-9.]/).map((step) => parseFloat(step));
    zoomSteps.sort((a, b) => a - b);

    // <img> object property variables
    let zoom = 1;
    let translateX = 0;
    let translateY = 0;
    let clickMouseX = undefined;
    let clickMouseY = undefined;
    let clickTranslateX = undefined;
    let clickTranslateY = undefined;

    updateNumberInfo();

    // update image numbers displayed in info box
    function updateNumberInfo() {
      numberInfoBox.innerHTML =
        sourceImg.dataset.number + " of " + sourceImg.dataset.total;
    }

    // update zoom displayed in info box
    function updateZoomInfo() {
      let zoomInfo = zoom * 100;
      if (!Number.isInteger(zoomInfo)) zoomInfo = zoomInfo.toFixed(2);
      zoomInfoBox.innerHTML = zoomInfo + "%";
    }

    // move to closest zoom step above current zoom
    const zoomIn = function () {
      for (const zoomStep of zoomSteps) {
        if (zoomStep > zoom) {
          zoom = zoomStep;
          break;
        }
      }
      updateTransform();
    };

    // move to closest zoom step above current zoom
    const zoomOut = function () {
      zoomSteps.reverse();
      for (const zoomStep of zoomSteps) {
        if (zoomStep < zoom) {
          zoom = zoomStep;
          break;
        }
      }
      zoomSteps.reverse();

      updateTransform();
    };

    // update display of <img> based on scale/translate properties
    const updateTransform = function () {
      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      // get new width/height after scale
      const rect = img.getBoundingClientRect();
      // limit translate
      translateX = Math.max(translateX, -rect.width / 2);
      translateX = Math.min(translateX, rect.width / 2);
      translateY = Math.max(translateY, -rect.height / 2);
      translateY = Math.min(translateY, rect.height / 2);

      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      updateZoomInfo();
    };

    // fit <img> to container
    const fit = function () {
      // no x/y offset, 100% zoom by default
      translateX = 0;
      translateY = 0;
      zoom = 1;

      // widths of <img> and container
      const imgWidth = img.naturalWidth;
      const imgHeight = img.naturalHeight;
      const containerWidth = parseFloat(
        window.getComputedStyle(container).width
      );
      const containerHeight = parseFloat(
        window.getComputedStyle(container).height
      );

      // how much zooming is needed to fit <img> to container
      const xRatio = imgWidth / containerWidth;
      const yRatio = imgHeight / containerHeight;
      const maxRatio = Math.max(xRatio, yRatio);
      const newZoom = 1 / maxRatio;

      // fit <img> to container according to option
      if (defaultZoom === "shrink") {
        if (maxRatio > 1) zoom = newZoom;
      } else if (defaultZoom === "fit") zoom = newZoom;

      updateTransform();
    };

    // when mouse wheel is rolled anywhere in container
    const onContainerWheel = function (event) {
      if (!event) return;

      // let ctrl + mouse wheel to zoom behave as normal
      if (event.ctrlKey) return;

      // prevent normal scroll behavior
      event.preventDefault();
      event.stopPropagation();

      // point around which to scale img
      const viewRect = container.getBoundingClientRect();
      const viewX = (viewRect.left + viewRect.right) / 2;
      const viewY = (viewRect.top + viewRect.bottom) / 2;
      const originX = centerZoom === "true" ? viewX : mouseX;
      const originY = centerZoom === "true" ? viewY : mouseY;

      // get point on image under origin
      const oldRect = img.getBoundingClientRect();
      const oldPercentX = (originX - oldRect.left) / oldRect.width;
      const oldPercentY = (originY - oldRect.top) / oldRect.height;

      // increment/decrement zoom
      if (event.deltaY < 0) zoomIn();
      if (event.deltaY > 0) zoomOut();

      // get offset between previous image point and origin
      const newRect = img.getBoundingClientRect();
      const offsetX = originX - (newRect.left + newRect.width * oldPercentX);
      const offsetY = originY - (newRect.top + newRect.height * oldPercentY);

      // translate image to keep image point under origin
      translateX += offsetX;
      translateY += offsetY;

      // perform translate
      updateTransform();
    };

    // when container is clicked
    function onContainerClick(event) {
      // if container itself is target of click, and not other
      // element above it
      if (event.target === this) closeLightbox();
    }

    // when mouse button is pressed on image
    const onImageMouseDown = function (event) {
      // store original mouse position relative to image
      clickMouseX = window.mouseX;
      clickMouseY = window.mouseY;
      clickTranslateX = translateX;
      clickTranslateY = translateY;
      event.stopPropagation();
      event.preventDefault();
    };

    // when mouse button is released anywhere in window
    const onWindowMouseUp = function (event) {
      // reset original mouse position
      clickMouseX = undefined;
      clickMouseY = undefined;
      clickTranslateX = undefined;
      clickTranslateY = undefined;

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mouseup", onWindowMouseUp);
    };

    // when mouse is moved anywhere in window
    const onWindowMouseMove = function (event) {
      if (
        clickMouseX === undefined ||
        clickMouseY === undefined ||
        clickTranslateX === undefined ||
        clickTranslateY === undefined
      )
        return;

      // offset image based on original and current mouse position
      translateX = clickTranslateX + window.mouseX - clickMouseX;
      translateY = clickTranslateY + window.mouseY - clickMouseY;
      updateTransform();
      event.preventDefault();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mousemove", onWindowMouseMove);
    };

    // when window is resized
    const onWindowResize = function (event) {
      fit();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("resize", onWindowResize);
    };

    // attach the necessary event listeners
    img.addEventListener("dblclick", fit);
    img.addEventListener("mousedown", onImageMouseDown);
    container.addEventListener("wheel", onContainerWheel);
    container.addEventListener("mousedown", onContainerClick);
    container.addEventListener("touchstart", onContainerClick);
    window.addEventListener("mouseup", onWindowMouseUp);
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("resize", onWindowResize);

    // run fit() after lightbox atttached to document and <img> Loaded
    // so needed container and img dimensions available
    img.addEventListener("load", fit);

    return img;
  }

  // make caption
  function makeCaption(img) {
    const caption = document.createElement("div");
    caption.id = "lightbox_caption";
    const captionSource = img.nextElementSibling;
    if (captionSource.tagName.toLowerCase() === "figcaption") {
      const captionCopy = makeCopy(captionSource);
      caption.innerHTML = captionCopy.innerHTML;
    }

    caption.addEventListener("touchstart", function (event) {
      event.stopPropagation();
    });

    return caption;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // make button to jump to previous image in document
  function makePrevButton(img) {
    const prevButton = document.createElement("button");
    prevButton.id = "lightbox_prev_button";
    prevButton.title = "Jump to the previous image in the document [←]";
    prevButton.classList.add("icon_button", "lightbox_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;

    // attach click listeners to button
    prevButton.addEventListener("click", function () {
      getPrevImg(img).click();
    });

    return prevButton;
  }

  // make button to jump to next image in document
  function makeNextButton(img) {
    const nextButton = document.createElement("button");
    nextButton.id = "lightbox_next_button";
    nextButton.title = "Jump to the next image in the document [→]";
    nextButton.classList.add("icon_button", "lightbox_button");
    nextButton.innerHTML = document.querySelector(
      ".icon_caret_right"
    ).innerHTML;

    // attach click listeners to button
    nextButton.addEventListener("click", function () {
      getNextImg(img).click();
    });

    return nextButton;
  }

  // get previous image in document
  function getPrevImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if < 1
    if (index - 1 >= 0) index--;
    else index = imgs.length - 1;
    return imgs[index];
  }

  // get next image in document
  function getNextImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if > total
    if (index + 1 <= imgs.length - 1) index++;
    else index = 0;
    return imgs[index];
  }

  // close lightbox
  function closeLightbox() {
    focusBody();

    const lightbox = document.getElementById("lightbox_overlay");
    if (lightbox) lightbox.remove();
  }

  // make all elements behind lightbox non-focusable
  function blurBody(overlay) {
    const all = document.querySelectorAll("*");
    for (const element of all) element.tabIndex = -1;
    document.body.classList.add("body_no_scroll");
  }

  // make all elements focusable again
  function focusBody() {
    const all = document.querySelectorAll("*");
    for (const element of all) element.removeAttribute("tabIndex");
    document.body.classList.remove("body_no_scroll");
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* regular <img> in document when hovered */
    img.lightbox_document_img:hover {
      cursor: pointer;
    }

    .body_no_scroll {
      overflow: hidden !important;
    }

    /* screen overlay */
    #lightbox_overlay {
      display: flex;
      flex-direction: column;
      position: fixed;
      left: 0;
      top: 0;
      right: 0;
      bottom: 0;
      background: rgba(0, 0, 0, 0.75);
      z-index: 3;
    }

    /* middle area containing lightbox image */
    #lightbox_image_container {
      flex-grow: 1;
      display: flex;
      justify-content: center;
      align-items: center;
      overflow: hidden;
      position: relative;
      padding: 20px;
    }

    /* bottom area containing caption */
    #lightbox_bottom_container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100px;
      min-height: 100px;
      max-height: 100px;
      background: rgba(0, 0, 0, 0.5);
    }

    /* image number info text box */
    #lightbox_number_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      left: 2px;
      top: 0;
      z-index: 4;
    }

    /* zoom info text box */
    #lightbox_zoom_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      right: 2px;
      top: 0;
      z-index: 4;
    }

    /* copy of image caption */
    #lightbox_caption {
      box-sizing: border-box;
      display: inline-block;
      width: 100%;
      max-height: 100%;
      padding: 10px 0;
      text-align: center;
      overflow-y: auto;
      color: #ffffff;
    }

    /* navigation previous/next button */
    .lightbox_button {
      width: 100px;
      height: 100%;
      min-width: 100px;
      min-height: 100%;
      color: #ffffff;
    }

    /* navigation previous/next button when hovered */
    .lightbox_button:hover {
      background: none !important;
    }

    /* navigation button icon */
    .lightbox_button > svg {
      height: 25px;
    }

    /* figure auto-number */
    #lightbox_caption > span:first-of-type {
      font-weight: bold;
      margin-right: 5px;
    }

    /* lightbox image when hovered */
    #lightbox_img:hover {
      cursor: grab;
    }

    /* lightbox image when grabbed */
    #lightbox_img:active {
      cursor: grabbing;
    }
  }

  /* when on screen < 480px wide */
  @media only screen and (max-width: 480px) {
    /* make navigation buttons skinnier on small screens to make more room for caption text */
    .lightbox_button {
      width: 50px;
      min-width: 50px;
    }
  }

  /* always hide lightbox on print */
  @media only print {
    #lightbox_overlay {
      display: none;
    }
  }
</style>
<!-- 
  Link Highlight Plugin

  Makes it such that when a user hovers or focuses a link, other links that have
  the same target will be highlighted. It also makes it such that when clicking
  a link, the target of the link (eg reference, figure, table) is briefly
  highlighted.
-->

<script type="module">
  // whether to also highlight links that go to external urls
  const externalLinks = "false";
  // whether user must click off to unhighlight instead of just
  // un-hovering
  const clickUnhighlight = "false";
  // whether to also highlight links that are unique
  const highlightUnique = "true";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach mouse and focus listeners to link
      link.addEventListener("mouseenter", onLinkFocus);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("mouseleave", onLinkUnhover);
    }

    // attach click and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("hashchange", onHashChange);

    // run hash change on window load in case user has navigated
    // directly to hash
    onHashChange();
  }

  // when link is focused (tabbed to) or hovered
  function onLinkFocus() {
    highlight(this);
  }

  // when link is unhovered
  function onLinkUnhover() {
    if (clickUnhighlight !== "true") unhighlightAll();
  }

  // when the mouse is clicked anywhere in window
  function onClick(event) {
    unhighlightAll();
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) glowElement(target);
  }

  // start glow sequence on an element
  function glowElement(element) {
    const startGlow = function () {
      onGlowEnd();
      element.dataset.glow = "true";
      element.addEventListener("animationend", onGlowEnd);
    };
    const onGlowEnd = function () {
      element.removeAttribute("data-glow");
      element.removeEventListener("animationend", onGlowEnd);
    };
    startGlow();
  }

  // highlight link and all others with same target
  function highlight(link) {
    // force unhighlight all to start fresh
    unhighlightAll();

    // get links with same target
    if (!link) return;
    const sameLinks = getSameLinks(link);

    // if link unique and option is off, exit and don't highlight
    if (sameLinks.length <= 1 && highlightUnique !== "true") return;

    // highlight all same links, and "select" (special highlight) this
    // one
    for (const sameLink of sameLinks) {
      if (sameLink === link) sameLink.setAttribute("data-selected", "true");
      else sameLink.setAttribute("data-highlighted", "true");
    }
  }

  // unhighlight all links
  function unhighlightAll() {
    const links = getLinks();
    for (const link of links) {
      link.setAttribute("data-selected", "false");
      link.setAttribute("data-highlighted", "false");
    }
  }

  // get links with same target
  function getSameLinks(link) {
    const results = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        results.push(otherLink);
    }
    return results;
  }

  // get all links of types we wish to handle
  function getLinks() {
    let query = "a";
    if (externalLinks !== "true") query += '[href^="#"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelectorAll(query);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<style>
  @media only screen {
    /* anything with data-highlighted attribute true */
    [data-highlighted="true"] {
      background: #ffeb3b;
    }

    /* anything with data-selected attribute true */
    [data-selected="true"] {
      background: #ff8a65 !important;
    }

    /* animation definition for glow */
    @keyframes highlight_glow {
      0% {
        background: none;
      }
      10% {
        background: #bbdefb;
      }
      100% {
        background: none;
      }
    }

    /* anything with data-glow attribute true */
    [data-glow="true"] {
      animation: highlight_glow 2s;
    }
  }
</style>
<!--
  Table of Contents Plugin

  Provides a "table of contents" (toc) panel on the side of the document that
  allows the user to conveniently navigate between sections of the document.
-->

<script type="module">
  // which types of elements to add links for, in "document.querySelector" format
  const typesQuery = "h1, h2, h3";
  // whether toc starts open. use 'true' or 'false', or 'auto' to
  // use 'true' behavior when screen wide enough and 'false' when not
  const startOpen = "false";
  // whether toc closes when clicking on toc link. use 'true' or
  // 'false', or 'auto' to use 'false' behavior when screen wide
  // enough and 'true' when not
  const clickClose = "auto";
  // if list item is more than this many characters, text will be
  // truncated
  const charLimit = "50";
  // whether or not to show bullets next to each toc item
  const bullets = "false";

  // start script
  function start() {
    // make toc panel and populate with entries (links to document
    // sections)
    const panel = makePanel();
    if (!panel) return;
    makeEntries(panel);
    // attach panel to document after making entries, so 'toc' heading
    // in panel isn't included in toc
    document.body.insertBefore(panel, document.body.firstChild);

    // initial panel state
    if (startOpen === "true" || (startOpen === "auto" && !isSmallScreen()))
      openPanel();
    else closePanel();

    // attach click, scroll, and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("scroll", onScroll);
    window.addEventListener("hashchange", onScroll);
    window.addEventListener("keyup", onKeyUp);
    onScroll();

    // add class to push document body down out of way of toc button
    document.body.classList.add("toc_body_nudge");
  }

  // determine if screen wide enough to fit toc panel
  function isSmallScreen() {
    // in default theme:
    // 816px = 8.5in = width of "page" (<body>) element
    // 260px = min width of toc panel (*2 for both sides of <body>)
    return window.innerWidth < 816 + 260 * 2;
  }

  // when mouse is clicked anywhere in window
  function onClick() {
    if (isSmallScreen()) closePanel();
  }

  // when window is scrolled or hash changed
  function onScroll() {
    highlightViewed();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    // close on esc
    if (event.key === "Escape") closePanel();
  }

  // find entry of currently viewed document section in toc and highlight
  function highlightViewed() {
    const firstId = getFirstInView(typesQuery);

    // get toc entries (links), unhighlight all, then highlight viewed
    const list = document.getElementById("toc_list");
    if (!firstId || !list) return;
    const links = list.querySelectorAll("a");
    for (const link of links) link.dataset.viewing = "false";
    const link = list.querySelector('a[href="#' + firstId + '"]');
    if (!link) return;
    link.dataset.viewing = "true";
  }

  // get first or previous toc listed element in top half of view
  function getFirstInView(query) {
    // get all elements matching query and with id
    const elements = document.querySelectorAll(query);
    const elementsWithIds = [];
    for (const element of elements) {
      if (element.id) elementsWithIds.push(element);
    }

    // get first or previous element in top half of view
    for (let i = 0; i < elementsWithIds.length; i++) {
      const element = elementsWithIds[i];
      const prevElement = elementsWithIds[Math.max(0, i - 1)];
      if (element.getBoundingClientRect().top >= 0) {
        if (element.getBoundingClientRect().top < window.innerHeight / 2)
          return element.id;
        else return prevElement.id;
      }
    }
  }

  // make panel
  function makePanel() {
    // create panel
    const panel = document.createElement("div");
    panel.id = "toc_panel";
    if (bullets === "true") panel.dataset.bullets = "true";

    // create header
    const header = document.createElement("div");
    header.id = "toc_header";

    // create toc button
    const button = document.createElement("button");
    button.id = "toc_button";
    button.innerHTML = document.querySelector(".icon_th_list").innerHTML;
    button.title = "Table of Contents";
    button.classList.add("icon_button");

    // create header text
    const text = document.createElement("h4");
    text.innerHTML = "Table of Contents";

    // create container for toc list
    const list = document.createElement("div");
    list.id = "toc_list";

    // attach click listeners
    panel.addEventListener("click", onPanelClick);
    header.addEventListener("click", onHeaderClick);
    button.addEventListener("click", onButtonClick);

    // attach elements
    header.appendChild(button);
    header.appendChild(text);
    panel.appendChild(header);
    panel.appendChild(list);

    return panel;
  }

  // create toc entries (links) to each element of the specified types
  function makeEntries(panel) {
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) {
      // do not add link if element doesn't have assigned id
      if (!element.id) continue;

      // create link/list item
      const link = document.createElement("a");
      link.classList.add("toc_link");
      switch (element.tagName.toLowerCase()) {
        case "h1":
          link.dataset.level = "1";
          break;
        case "h2":
          link.dataset.level = "2";
          break;
        case "h3":
          link.dataset.level = "3";
          break;
        case "h4":
          link.dataset.level = "4";
          break;
      }
      link.title = element.innerText;
      let text = element.innerText;
      if (text.length > charLimit) text = text.slice(0, charLimit) + "...";
      link.innerHTML = text;
      link.href = "#" + element.id;
      link.addEventListener("click", onLinkClick);

      // attach link
      panel.querySelector("#toc_list").appendChild(link);
    }
  }

  // when panel is clicked
  function onPanelClick(event) {
    // stop click from propagating to window/document and closing panel
    event.stopPropagation();
  }

  // when header itself is clicked
  function onHeaderClick(event) {
    togglePanel();
  }

  // when button is clicked
  function onButtonClick(event) {
    togglePanel();
    // stop header underneath button from also being clicked
    event.stopPropagation();
  }

  // when link is clicked
  function onLinkClick(event) {
    if (clickClose === "true" || (clickClose === "auto" && isSmallScreen()))
      closePanel();
    else openPanel();
  }

  // open panel if closed, close if opened
  function togglePanel() {
    const panel = document.getElementById("toc_panel");
    if (!panel) return;

    if (panel.dataset.open === "true") closePanel();
    else openPanel();
  }

  // open panel
  function openPanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "true";
  }

  // close panel
  function closePanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "false";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- th list icon -->

<template class="icon_th_list">
  <!-- modified from: https://fontawesome.com/icons/th-list -->
  <svg width="16" height="16" viewBox="0 0 512 512" tabindex="-1">
    <path
      fill="currentColor"
      d="M96 96c0 26.51-21.49 48-48 48S0 122.51 0 96s21.49-48 48-48 48 21.49 48 48zM48 208c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm0 160c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm96-236h352c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* toc panel */
    #toc_panel {
      box-sizing: border-box;
      position: fixed;
      top: 0;
      left: 0;
      background: #ffffff;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      z-index: 2;
    }

    /* toc panel when closed */
    #toc_panel[data-open="false"] {
      min-width: 60px;
      width: 60px;
      height: 60px;
      border-right: solid 1px #bdbdbd;
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc panel when open */
    #toc_panel[data-open="true"] {
      min-width: 260px;
      max-width: 480px;
      /* keep panel edge consistent distance away from "page" edge */
      width: calc(((100vw - 8.5in) / 2) - 30px - 40px);
      bottom: 0;
      border-right: solid 1px #bdbdbd;
    }

    /* toc panel header */
    #toc_header {
      box-sizing: border-box;
      display: flex;
      flex-direction: row;
      align-items: center;
      height: 60px;
      margin: 0;
      padding: 20px;
    }

    /* toc panel header when hovered */
    #toc_header:hover {
      cursor: pointer;
    }

    /* toc panel header when panel open */
    #toc_panel[data-open="true"] > #toc_header {
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc open/close header button */
    #toc_button {
      margin-right: 20px;
    }

    /* hide toc list and header text when closed */
    #toc_panel[data-open="false"] > #toc_header > *:not(#toc_button),
    #toc_panel[data-open="false"] > #toc_list {
      display: none;
    }

    /* toc list of entries */
    #toc_list {
      box-sizing: border-box;
      width: 100%;
      padding: 20px;
      position: absolute;
      top: calc(60px + 1px);
      bottom: 0;
      overflow: auto;
    }

    /* toc entry, link to section in document */
    .toc_link {
      display: block;
      padding: 5px;
      position: relative;
      font-weight: 600;
      text-decoration: none;
    }

    /* toc entry when hovered or when "viewed" */
    .toc_link:hover,
    .toc_link[data-viewing="true"] {
      background: #f5f5f5;
    }

    /* toc entry, level 1 indentation */
    .toc_link[data-level="1"] {
      margin-left: 0;
    }

    /* toc entry, level 2 indentation */
    .toc_link[data-level="2"] {
      margin-left: 20px;
    }

    /* toc entry, level 3 indentation */
    .toc_link[data-level="3"] {
      margin-left: 40px;
    }

    /* toc entry, level 4 indentation */
    .toc_link[data-level="4"] {
      margin-left: 60px;
    }

    /* toc entry bullets */
    #toc_panel[data-bullets="true"] .toc_link[data-level]:before {
      position: absolute;
      left: -15px;
      top: -1px;
      font-size: 1.5em;
    }

    /* toc entry, level 2 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="2"]:before {
      content: "\2022";
    }

    /* toc entry, level 3 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="3"]:before {
      content: "\25AB";
    }

    /* toc entry, level 4 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="4"]:before {
      content: "-";
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* push <body> ("page") element down to make room for toc icon */
    .toc_body_nudge {
      padding-top: 60px;
    }

    /* toc icon when panel closed and not hovered */
    #toc_panel[data-open="false"]:not(:hover) {
      background: rgba(255, 255, 255, 0.75);
    }
  }

  /* always hide toc panel on print */
  @media only print {
    #toc_panel {
      display: none;
    }
  }
</style>
<!-- 
  Tooltips Plugin

  Makes it such that when the user hovers or focuses a link to a citation or
  figure, a tooltip appears with a preview of the reference content, along with
  arrows to navigate between instances of the same reference in the document.
-->

<script type="module">
  // whether user must click off to close tooltip instead of just un-hovering
  const clickClose = "false";
  // delay (in ms) between opening and closing tooltip
  const delay = "100";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach hover and focus listeners to link
      link.addEventListener("mouseover", onLinkHover);
      link.addEventListener("mouseleave", onLinkUnhover);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("touchend", onLinkTouch);
    }

    // attach mouse, key, and resize listeners to window
    window.addEventListener("mousedown", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("keyup", onKeyUp);
    window.addEventListener("resize", onResize);
  }

  // when link is hovered
  function onLinkHover() {
    // function to open tooltip
    const delayOpenTooltip = function () {
      openTooltip(this);
    }.bind(this);

    // run open function after delay
    this.openTooltipTimer = window.setTimeout(delayOpenTooltip, delay);
  }

  // when mouse leaves link
  function onLinkUnhover() {
    // cancel opening tooltip
    window.clearTimeout(this.openTooltipTimer);

    // don't close on unhover if option specifies
    if (clickClose === "true") return;

    // function to close tooltip
    const delayCloseTooltip = function () {
      // if tooltip open and if mouse isn't over tooltip, close
      const tooltip = document.getElementById("tooltip");
      if (tooltip && !tooltip.matches(":hover")) closeTooltip();
    };

    // run close function after delay
    this.closeTooltipTimer = window.setTimeout(delayCloseTooltip, delay);
  }

  // when link is focused (tabbed to)
  function onLinkFocus(event) {
    openTooltip(this);
  }

  // when link is touched on touch screen
  function onLinkTouch(event) {
    // attempt to force hover state on first tap always, and trigger
    // regular link click (and navigation) on second tap
    if (event.target === document.activeElement) event.target.click();
    else {
      document.activeElement.blur();
      event.target.focus();
    }
    if (event.cancelable) event.preventDefault();
    event.stopPropagation();
    return false;
  }

  // when mouse is clicked anywhere in window
  function onClick(event) {
    closeTooltip();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("tooltip_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("tooltip_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeTooltip();
        break;
    }
  }

  // when window is resized or zoomed
  function onResize() {
    closeTooltip();
  }

  // get all links of types we wish to handle
  function getLinks() {
    const queries = [];
    // exclude buttons, anchor links, toc links, etc
    const exclude =
      ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    queries.push('a[href^="#ref-"]' + exclude); // citation links
    queries.push('a[href^="#fig:"]' + exclude); // figure links
    const query = queries.join(", ");
    return document.querySelectorAll(query);
  }

  // get links with same target, get index of link in set, get total
  // same links
  function getSameLinks(link) {
    const sameLinks = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        sameLinks.push(otherLink);
    }

    return {
      elements: sameLinks,
      index: sameLinks.indexOf(link),
      total: sameLinks.length,
    };
  }

  // open tooltip
  function openTooltip(link) {
    // delete tooltip if it exists, start fresh
    closeTooltip();

    // make tooltip element
    const tooltip = makeTooltip(link);

    // if source couldn't be found and tooltip not made, exit
    if (!tooltip) return;

    // make navbar elements
    const navBar = makeNavBar(link);
    if (navBar) tooltip.firstElementChild.appendChild(navBar);

    // attach tooltip to page
    document.body.appendChild(tooltip);

    // position tooltip
    const position = function () {
      positionTooltip(link);
    };
    position();

    // if tooltip contains images, position again after they've loaded
    const imgs = tooltip.querySelectorAll("img");
    for (const img of imgs) img.addEventListener("load", position);
  }

  // close (delete) tooltip
  function closeTooltip() {
    const tooltip = document.getElementById("tooltip");
    if (tooltip) tooltip.remove();
  }

  // make tooltip
  function makeTooltip(link) {
    // get target element that link points to
    const source = getSource(link);

    // if source can't be found, exit
    if (!source) return;

    // create new tooltip
    const tooltip = document.createElement("div");
    tooltip.id = "tooltip";
    const tooltipContent = document.createElement("div");
    tooltipContent.id = "tooltip_content";
    tooltip.appendChild(tooltipContent);

    // make copy of source node and put in tooltip
    const sourceCopy = makeCopy(source);
    tooltipContent.appendChild(sourceCopy);

    // attach mouse event listeners
    tooltip.addEventListener("click", onTooltipClick);
    tooltip.addEventListener("mousedown", onTooltipClick);
    tooltip.addEventListener("touchstart", onTooltipClick);
    tooltip.addEventListener("mouseleave", onTooltipUnhover);

    // (for interaction with lightbox plugin)
    // transfer click on tooltip copied img to original img
    const sourceImg = source.querySelector("img");
    const sourceCopyImg = sourceCopy.querySelector("img");
    if (sourceImg && sourceCopyImg) {
      const clickImg = function () {
        sourceImg.click();
        closeTooltip();
      };
      sourceCopyImg.addEventListener("click", clickImg);
    }

    return tooltip;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
      "class",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // when tooltip is clicked
  function onTooltipClick(event) {
    // when user clicks on tooltip, stop click from transferring
    // outside of tooltip (eg, click off to close tooltip, or eg click
    // off to unhighlight same refs)
    event.stopPropagation();
  }

  // when tooltip is unhovered
  function onTooltipUnhover(event) {
    if (clickClose === "true") return;

    // make sure new mouse/touch/focus no longer over tooltip or any
    // element within it
    const tooltip = document.getElementById("tooltip");
    if (!tooltip) return;
    if (this.contains(event.relatedTarget)) return;

    closeTooltip();
  }

  // make nav bar to go betwen prev/next instances of same reference
  function makeNavBar(link) {
    // find other links to the same source
    const sameLinks = getSameLinks(link);

    // don't show nav bar when singular reference
    if (sameLinks.total <= 1) return;

    // find prev/next links with same target
    const prevLink = getPrevLink(link, sameLinks);
    const nextLink = getNextLink(link, sameLinks);

    // create nav bar
    const navBar = document.createElement("div");
    navBar.id = "tooltip_nav_bar";
    const text = sameLinks.index + 1 + " of " + sameLinks.total;

    // create nav bar prev/next buttons
    const prevButton = document.createElement("button");
    const nextButton = document.createElement("button");
    prevButton.id = "tooltip_prev_button";
    nextButton.id = "tooltip_next_button";
    prevButton.title =
      "Jump to the previous occurence of this item in the document [←]";
    nextButton.title =
      "Jump to the next occurence of this item in the document [→]";
    prevButton.classList.add("icon_button");
    nextButton.classList.add("icon_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;
    nextButton.innerHTML =
      document.querySelector(".icon_caret_right").innerHTML;
    navBar.appendChild(prevButton);
    navBar.appendChild(document.createTextNode(text));
    navBar.appendChild(nextButton);

    // attach click listeners to buttons
    prevButton.addEventListener("click", function () {
      onPrevNextClick(link, prevLink);
    });
    nextButton.addEventListener("click", function () {
      onPrevNextClick(link, nextLink);
    });

    return navBar;
  }

  // get previous link with same target
  function getPrevLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if < 1
    let index;
    if (sameLinks.index - 1 >= 0) index = sameLinks.index - 1;
    else index = sameLinks.total - 1;
    return sameLinks.elements[index];
  }

  // get next link with same target
  function getNextLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if > total
    let index;
    if (sameLinks.index + 1 <= sameLinks.total - 1) index = sameLinks.index + 1;
    else index = 0;
    return sameLinks.elements[index];
  }

  // get element that is target of link or url hash
  function getSource(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector('[id="' + id + '"]');
    if (!target) return;

    // if ref or figure, modify target to get expected element
    if (id.indexOf("ref-") === 0) target = target.querySelector(":nth-child(2)");
    else if (id.indexOf("fig:") === 0) target = target.querySelector("figure");

    return target;
  }

  // when prev/next arrow button is clicked
  function onPrevNextClick(link, prevNextLink) {
    if (link && prevNextLink)
      goToElement(prevNextLink, window.innerHeight * 0.5);
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);
    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // determine position to place tooltip based on link position in
  // viewport and tooltip size
  function positionTooltip(link, left, top) {
    const tooltipElement = document.getElementById("tooltip");
    if (!tooltipElement) return;

    // get convenient vars for position/dimensions of
    // link/tooltip/page/view
    link = getRectInPage(link);
    const tooltip = getRectInPage(tooltipElement);
    const view = getRectInPage();

    // horizontal positioning
    if (left)
      // use explicit value
      left = left;
    else if (link.left + tooltip.width < view.right)
      // fit tooltip to right of link
      left = link.left;
    else if (link.right - tooltip.width > view.left)
      // fit tooltip to left of link
      left = link.right - tooltip.width;
    // center tooltip in view
    else left = (view.right - view.left) / 2 - tooltip.width / 2;

    // vertical positioning
    if (top)
      // use explicit value
      top = top;
    else if (link.top - tooltip.height > view.top)
      // fit tooltip above link
      top = link.top - tooltip.height;
    else if (link.bottom + tooltip.height < view.bottom)
      // fit tooltip below link
      top = link.bottom;
    else {
      // center tooltip in view
      top = view.top + view.height / 2 - tooltip.height / 2;
      // nudge off of link to left/right if possible
      if (link.right + tooltip.width < view.right) left = link.right;
      else if (link.left - tooltip.width > view.left)
        left = link.left - tooltip.width;
    }

    tooltipElement.style.left = left + "px";
    tooltipElement.style.top = top + "px";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* tooltip container */
    #tooltip {
      position: absolute;
      width: 50%;
      min-width: 240px;
      max-width: 75%;
      z-index: 1;
    }

    /* tooltip content */
    #tooltip_content {
      margin-bottom: 5px;
      padding: 20px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
      overflow-wrap: break-word;
    }

    /* tooltip copy of paragraphs and figures */
    #tooltip_content > p,
    #tooltip_content > figure {
      margin: 0;
      max-height: 320px;
      overflow-y: auto;
    }

    /* tooltip copy of <img> */
    #tooltip_content > figure > img,
    #tooltip_content > figure > svg {
      max-height: 260px;
    }

    /* navigation bar */
    #tooltip_nav_bar {
      margin-top: 10px;
      text-align: center;
    }

    /* navigation bar previous/next buton */
    #tooltip_nav_bar > .icon_button {
      position: relative;
      top: 3px;
    }

    /* navigation bar previous button */
    #tooltip_nav_bar > .icon_button:first-of-type {
      margin-right: 5px;
    }

    /* navigation bar next button */
    #tooltip_nav_bar > .icon_button:last-of-type {
      margin-left: 5px;
    }
  }

  /* always hide tooltip on print */
  @media only print {
    #tooltip {
      display: none;
    }
  }
</style>
<!--
  Analytics Plugin (third-party) 
  
  Copy and paste code from Google Analytics or similar service here.
-->
<!-- 
  Annotations Plugin

  Allows public annotation of the  manuscript. See https://web.hypothes.is/.
-->

<script>
  // configuration
  window.hypothesisConfig = function () {
    return {
      branding: {
        accentColor: "#2196f3",
        appBackgroundColor: "#f8f8f8",
        ctaBackgroundColor: "#f8f8f8",
        ctaTextColor: "#000000",
        selectionFontFamily: "Open Sans, Helvetica, sans serif",
        annotationFontFamily: "Open Sans, Helvetica, sans serif",
      },
    };
  };

  // hypothesis client script
  const embed = "https://hypothes.is/embed.js";
  // hypothesis annotation count query url
  const query = "https://api.hypothes.is/api/search?limit=0&url=";

  // start script
  function start() {
    const button = makeButton();
    document.body.insertBefore(button, document.body.firstChild);
    insertCount(button);
  }

  // make button
  function makeButton() {
    // create button
    const button = document.createElement("button");
    button.id = "hypothesis_button";
    button.innerHTML = document.querySelector(".icon_hypothesis").innerHTML;
    button.title = "Hypothesis annotations";
    button.classList.add("icon_button");

    function onClick(event) {
      onButtonClick(event, button);
    }

    // attach click listeners
    button.addEventListener("click", onClick);

    return button;
  }

  // insert annotations count
  async function insertCount(button) {
    // get annotation count from Hypothesis based on url
    let count = "-";
    try {
      const canonical = document.querySelector('link[rel="canonical"]');
      const location = window.location;
      const url = encodeURIComponent((canonical || location).href);
      const response = await fetch(query + url);
      const json = await response.json();
      count = json.total || "-";
    } catch (error) {
      console.log(error);
    }

    // put count into button
    const counter = document.createElement("span");
    counter.id = "hypothesis_count";
    counter.innerHTML = count;
    button.title = "View " + count + " Hypothesis annotations";
    button.append(counter);
  }

  // when button is clicked
  function onButtonClick(event, button) {
    const script = document.createElement("script");
    script.src = embed;
    document.body.append(script);
    button.remove();
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- hypothesis icon -->

<template class="icon_hypothesis">
  <!-- modified from: https://simpleicons.org/icons/hypothesis.svg / https://git.io/Jf1VB -->
  <svg width="16" height="16" viewBox="0 0 24 24" tabindex="-1">
    <path
      fill="currentColor"
      d="M3.43 0C2.5 0 1.72 .768 1.72 1.72V18.86C1.72 19.8 2.5 20.57 3.43 20.57H9.38L12 24L14.62 20.57H20.57C21.5 20.57 22.29 19.8 22.29 18.86V1.72C22.29 .77 21.5 0 20.57 0H3.43M5.14 3.43H7.72V9.43S8.58 7.72 10.28 7.72C12 7.72 13.74 8.57 13.74 11.24V17.14H11.16V12C11.16 10.61 10.28 10.07 9.43 10.29C8.57 10.5 7.72 11.41 7.72 13.29V17.14H5.14V3.43M18 13.72C18.95 13.72 19.72 14.5 19.72 15.42A1.71 1.71 0 0 1 18 17.13A1.71 1.71 0 0 1 16.29 15.42C16.29 14.5 17.05 13.71 18 13.71Z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  /* hypothesis activation button */
  #hypothesis_button {
    box-sizing: border-box;
    position: fixed;
    top: 0;
    right: 0;
    width: 60px;
    height: 60px;
    background: #ffffff;
    border-radius: 0;
    border-left: solid 1px #bdbdbd;
    border-bottom: solid 1px #bdbdbd;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
    z-index: 2;
  }

  /* hypothesis button svg */
  #hypothesis_button > svg {
    position: relative;
    top: -4px;
  }

  /* hypothesis annotation count */
  #hypothesis_count {
    position: absolute;
    left: 0;
    right: 0;
    bottom: 5px;
  }

  /* side panel */
  .annotator-frame {
    width: 280px !important;
  }

  /* match highlight color to rest of theme */
  .annotator-highlights-always-on .annotator-hl {
    background-color: #ffeb3b !important;
  }

  /* match focused color to rest of theme */
  .annotator-hl.annotator-hl-focused {
    background-color: #ff8a65 !important;
  }

  /* match bucket bar color to rest of theme */
  .annotator-bucket-bar {
    background: #f5f5f5 !important;
  }

  /* always hide button, toolbar, and tooltip on print */
  @media only print {
    #hypothesis_button {
      display: none;
    }

    .annotator-frame {
      display: none !important;
    }

    hypothesis-adder {
      display: none !important;
    }
  }
</style>
<!-- 
  Mathjax Plugin (third-party) 

  Allows the proper rendering of math/equations written in LaTeX.
  See https://www.mathjax.org/.
-->

<script type="text/x-mathjax-config">
  // configuration
  MathJax.Hub.Config({
    "CommonHTML": { linebreaks: { automatic: true } },
    "HTML-CSS": { linebreaks: { automatic: true } },
    "SVG": { linebreaks: { automatic: true } },
    "fast-preview": { disabled: true }
  });
</script>

<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A=="
  crossorigin="anonymous"
></script>

<style>
  /* mathjax containers */
  .math.display > span:not(.MathJax_Preview) {
    /* turn inline element (no dimensions) into block (allows fixed width and thus scrolling) */
    display: flex !important;
    overflow-x: auto !important;
    overflow-y: hidden !important;
    justify-content: center;
    align-items: center;
    margin: 0 !important;
  }

  /* right click menu */
  .MathJax_Menu {
    border-radius: 5px !important;
    border: solid 1px #bdbdbd !important;
    box-shadow: none !important;
  }

  /* equation auto-number */
  span[id^="eq:"] > span.math.display + span {
    font-weight: 600;
  }

  /* equation */
  span[id^="eq:"] > span.math.display > span {
    /* nudge to make room for equation auto-number and anchor */
    margin-right: 60px !important;
  }
</style>
<!-- 
  Scite Plugin (third party)

  Adds a Scite badge next to each citation in the references section.
  See https://scite.ai/.
-->

<script
  async
  type="application/javascript"
  src="https://cdn.scite.ai/badge/scite-badge-latest.min.js"
></script>

<script>
  // start script
  function start() {
    // if printing, exit and don't run badges
    if (window.matchMedia("print").matches) return;

    // get citation elements
    const query = ".references div[id*='ref-']";
    const elements = document.querySelectorAll(query);
    for (const element of elements) addBadge(element);
  }

  // add badge in citation element
  function addBadge(element) {
    // get citation text
    const text = element.innerText;
    // find doi string, eg "DOI: xxxxx/xxxxx"
    const match = text.match(/DOI:\s+(\S*)/) || {};
    const doi = match[1];
    if (!doi) return;
    // create cite badge and put after citation
    const badge = document.createElement("div");
    badge.innerHTML = "Loading Scite badge...";
    badge.classList.add("scite-badge");
    badge.setAttribute("data-doi", doi);
    badge.setAttribute("data-layout", "horizontal");
    badge.setAttribute("data-show-zero", "true");
    element.append(badge);
    // modify preceding citation <p> element
    const p = element.querySelector("p");
    if (p) p.style.marginBottom = "0";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<style>
  .scite-badge {
    text-indent: 0;
    margin-top: 10px;
  }

  @media print {
    .scite-badge {
      display: none;
    }
  }
</style>
</body>
</html>
